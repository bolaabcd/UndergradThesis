\textbf{Odds} are the fraction of probabilities. \textbf{Prior (predictive/prospective) odds} is $\frac{p(H)}{p(\neg H)}$, and the \textbf{Posterior (diagnostic/retrospective) odds} is $\frac{p(H|e)}{p(\neg H|e)}$. This is how much more likely the hypothesis is to be true than false a priori and after observing the event $e$. 

The \textbf{Likelyhood Ratio (Risk Ratio for epidemology)} is $\frac{p(e|H)}{p(e|\neg H)}$, remembering that Likelyhood is a function of $B$ in $p(A|B)$, while the probability is a function of $A$.

The formula is: Posterior Odds = Prior Odds $\times$ Likelyhood Ratio.

My interpretation of $p(H|e)$ is the probability we give to $H$ in the world where $e$ happens, thus if we do $\frac{p(H|e)}{\neg H|e}$ we're seeing how more probable (multiplicatively) $H$ is to be true in this world, and if we do $\frac{p(e|H)}{p(e|\neg H)}$ we're seeing how much more likely is the event $e$ to happen in the world in which $H$ is true than in the world in which it's not (it's a comparison accros worlds).

I interpret the likelyhood ratio as how many more times the evidence appears in the world where $H$ is true than in the world where it's not.

So how more likely the hypothesis is to be true than false, after we observe the event = how more likely the hypothesis was to be true before the observation was made times $\times$ how many more times the evidence appears in the world where $H$ is true than in the world where it's not.

\textbf{Odds of hypothesis after $e$ = odds before $e$ $\times$ how much more $e$ happens in $H$ than in $\neg H$.}

Covariance is the expected value of $(X-E[X])(Y-E[Y])$, distance to the averages, $cov(X,X) = var(X) = (std(X))^2$, and $corr(X,Y) = \frac{cov(X,Y)}{std(X)std(Y)}$.

Regression coefficient when estimating $Y$ using $X$ is $corr(X,Y)\times \frac{std(Y)}{std(X)}$, which is how much $Y$ will change by unity of $X$ we change, if we use the line that minimizes the quadratic error of the $Y$ estimate. I kind of interpret this as $\frac{(\text{X-unities})}{(\text{X-unities per standard devition of X})} \times corr(X,Y) \times std(Y) = (\text{number of standard deviations of X}) \times corr(X,Y) \times std(Y) = (\text{number of standard deviations of Y}) \times (\text{Y-unities per standard deviations of Y}) = (\text{Y-unities})$. The strange thing with this interpreatation is that $corr(X,Y) = (\frac{\text{standard deviations of X}}{\text{standard devations of Y}}) = \frac{\text{standard deviations of Y}}{\text{standard deviations of X}}$, is the function that given one ammount of standard deviations returns the other one... Maybe this is a reflection of the limitations of the linearity assumption?

Finally, the graphoid axioms for independence of random variables (all of them conditioned on $Z$, and I simplified a little bit):

\begin{enumerate}
	\item Symmetry: $X$ is independent of $Y$ iff $Y$ is independent of $X$
	\item Decomposition, Weak Union and Contraction: $X$ is independent of $YW$ iff (($X$ is independent of $Y$) and ($X$ is independent of $W$ conditional on $Y$)).
	\item 
\end{enumerate}

Os axiomas são (todos condicionado para um Z qualquer, versão simplificada por mim):

1) Simetria: X independente de Y <=> Y independente de X

2) Decomposição, Weak Union, Contraction: X independente de YW se e somente se ((X independente de Y) e (X independente de W condicional a Y))

Isso tudo sse ((X independente de W) e (X independente de Y condicional a W)), só trocar o nome de Y e W, a YW = WY aqui...

Esse acima implica decomposição diretamente, implica Weak Union tbm diretamente, e implica contraction diretamente tbm. Os três juntos levam a esse bem direto tbm, são realmente equivalentes.

3) Intersection (só se não tem nada com chance 0, strictly positive distributions): X independente de W dado Y e X independente de Y dado W implica que X independente de WY. Acho que é um sse tbm pelos outros axiomas, mas ele fala que essa ida só é válida em distribuições estritamente positivas.

Em resumo: 1) independência é simétrica, 2) independente de duas coisas é o mesmo que independente de uma e independente da outra dado a primeira / saber y,w não ajuda na minha estimativa de x <=> y sozinho não ajuda e w não muda se eu já sei y, e 3) independente das duas é o mesmo que ser independente de uma dado a outra e da outra dado a uma (vale só pra distribuições estritamente positivas aparentemente).


NOTAS NO PAPEL PASSAR PRO PC
