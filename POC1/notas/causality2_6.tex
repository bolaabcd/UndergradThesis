A \textit{projection} of a latent structure $L$ is another latent structure $L_O$ ($O$ are the observables) that keeps all independencies from $L$ (for every stable distribution generated by $L$) and in which each unobserved has two children (we omit the unobserveds and just connnect the two children with an undirected edge).

I didn't understand exactly what the first few phrases of this section mean... 

The distributions are not unlikely anymore to have extra independencies? (maybe this is just saying that the unobserved variables create/remove some independencies, so the observed ones might not even have a DAG structure )

Also, why does he say that we can \q{no longer guarantee} a DAG structure among the minimal compatible latent structures? Before this point, we already could find a case in which we didn't find directions for all edges, right? We just found the \q{maximally directed} graph...

Also, why does the instability of the distribution imply that we we can't find a DAG structure???

Maybe he just means that it might really be the case that there might be no DAG consistent with the data?? (and minimal)? Not having stability imples that we can not guarantee that the independencies represent stuff in the graph, and therefore we might find only non-DAG minimal structures???? Something like we have one variable that acts as a mediator of some nodes and as a confounder of parents of these nodes, somehow???? So we can not assign a direction? This is super weird, I'm not sure that's what the book meant...

I also didn't understand very well how the projection helped, as they use stable distributions (are the stable distributions in the definition of projection not stable on O but stable on all variables?)...

Are we trying to find a projection of the complete model in respect to the observables, then trying to find a causal link on one projection on the minimal model implies finding it on every minimal model???? And what is a distinguished projection? 

Also, I think that why the algorithm works is really not obvious, as Pearl says the original proof that it worked.


In the end, what I got is: we have an algorithm that identifies as much as it's possible to, the directions of the edges and whether we have an unobservable confounding, effectively buiding all we can gather from a distribution. Assuming that the distribution is somewhat stable (in respect to the whole structure...), that no small changes of parameters would change independencies.

This difference between stability with respect to the observables and with respect to the whole structure is still strange to me, but I'll assume that the stability with respect to the observables means just that we can't act as if the unobservables weren't there, we need to consider the possibility of latent variables interferring. If we do, we would be assuming that there is no extra independency besides the ones induced by the arrows between the observables, but some of these independencies might not be even present in the usual distribution generated by the complete model, and maybe (who knows), some might appear (two dependent variables are seem as independent, we would need to be able to make two dependent variables independent by adding edges and nodes, this seems impossible because the DAG-independency is just \q{there is no unblocked path}, so if there is a path we would need to remove it otherwise we cant make the variables un-independent!).
