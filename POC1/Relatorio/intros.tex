\subsection{Machine Learning}

Machine Learning is the field of study that focuses on developing methods of learning patterns from data in a way that generalizes to situations not present in the data. In recent years, significative advances have been observed in Machine Learning research, and also the popularity and applications of some methods increased significantly. One example is the improvement of Neural Network Architectures\cite{?}, and the popularity of Generative Models\cite{?}. Also, some recent research is focused on the theory behind such machine learning methods\cite{livros de SAAMAP}, and statistical learning in general \cite{VladmirNaumovichVapnik}. Part of the goal of the formalization of such theory is to provide more qualitative guarantees, for instance, in regard not only to accuracy, but also fairness, privacy and interpretability. We discuss these different goals in the next four subsections.

In general, we consider \textbf{supervised learning} problems: a machine learning model is an algorithm that receives many data points, which we call \textbf{training data}, and outputs a model. This model is itself an algorithm that receives a data point without the target value and outputs a prediction of the target value, which represents one or more variable of interest. This is callled supervised learning because the algorithm has acces to the target variable during the training process.

\subsection{Accuracy}

Accuracy is the notion of how close some estimate is to the true value we are estimating. In the context of Machine Learning, it represents how close the predictions of a given model are to the real value of the variable the model aims to predict. For binary classification (the scenario in which the target value has only two possible values), accuracy is defined in machine learning as $\frac{TP+TN}{TP+TN+FP+FN}$, where:

\begin{enumerate}
\item $TP$ is the number of True Positives: how many predictions were labeled as True and were really True.
\item $TN$ is the number of True Positives: how many predictions were labeled as True but were actually False.
\item $FP$ is the number of True Positives: how many predictions were labeled as False and were really False.
\item $FN$ is the number of True Positives: how many predictions were labeled as False but were actually True.
\end{enumerate}

So, the usual notion of accuracy is the proportion of the predictions from the model that were correct. This generalizes to multiclass classification problems (in which the target variable has a finite number of possible values) by considering the proportion of times that the model's prediction was correct. Regression problems are the ones in which the target variable has an infinite number of possible values but can be codified as a vector of numbers, for instance, the value of some building at two different times. We can access how accurate a regression model is in many ways, for instance the square difference between the prediction value and the real value, added through all training data points.

One important point to consider is that it is better to evaluate how accurate the system is with data different from the data used for training. This is because during the training phase the Machine Learning algorithm usually has the goal of providing the model that provides the best possible accuracy in the training data, among the possible models supported. If there is enough freedom among the possible models, then it might be possible to obtain a model that has a very good accuracy, but if we try to use this same model on other data, this same model performs very badly. For instance, if all functions from the training data to the output are allowed, then a model that simply memorizes the training data and outputs the correct result by looking at the memorized data and outputs a random answer if the input is not in the training data will have one hundred percent accuracy in the training data, but we can obviously provide no guarantee for data points not present. This problem is called \textbf{overfitting}, and is usually avoided by limiting how powerfull the model can be, in conjuction with verifying real accuracy values with data other than the training data, and we call this the \textbf{testing data}. Also, notice that to evaluate the model, the test data should also have the target value of each data point.

The classical goal of Machine Learning is to provide models with good \textit{test accuracy}, and an Machine Learning algorithm that produces models such that good results on the training data reflet on good results on testing data are said to \textbf{generalize} well. However, with the groth of Machine Learning applications in real-life scenarios and the impact on society as a whole, there has been a crescent focus on aspects other than simply maximizing the accuracy, in a similar way that we usually are worried only about how fast a cryptographic algorithm is but also about how safe it is.

\subsection{Fairness}



\subsection{Privacy}



\subsection{Interpretability}



\subsection{Causality}



\subsection{Quantitative Information Flow}



