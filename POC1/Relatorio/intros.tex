\subsection{Machine Learning}

Machine Learning is the field of study that focuses on developing methods of learning patterns from data in a way that generalizes to situations not present in the data. In recent years, significative advances have been observed in Machine Learning research, and also the popularity and applications of some methods increased significantly. One example is the improvement of Neural Network Architectures\cite{?}, and the popularity of Generative Models\cite{?}. Also, some recent research is focused on the theory behind such machine learning methods\cite{livros de SAAMAP}, and statistical learning in general \cite{VladmirNaumovichVapnik}. Part of the goal of the formalization of such theory is to provide more qualitative guarantees, for instance, in regard not only to accuracy, but also fairness, privacy and interpretability. We discuss these different goals in the next four subsections.

In general, we consider \textbf{supervised learning} problems: a machine learning model is an algorithm that receives many data points, which we call \textbf{training data}, and outputs a model. This model is itself an algorithm that receives a data point without the target value and outputs a prediction of the target value, which represents one or more variable of interest. This is callled supervised learning because the algorithm has acces to the target variable during the training process.

\subsection{Accuracy}

Accuracy is the notion of how close some estimate is to the true value we are estimating. In the context of Machine Learning, it represents how close the predictions of a given model are to the real value of the variable the model aims to predict. For binary classification (the scenario in which the target value has only two possible values), accuracy is defined in machine learning as $\frac{TP+TN}{TP+TN+FP+FN}$, where:

\begin{enumerate}
\item $TP$ is the number of True Positives: how many predictions were labeled as True and were really True.
\item $TN$ is the number of True Positives: how many predictions were labeled as True but were actually False.
\item $FP$ is the number of True Positives: how many predictions were labeled as False and were really False.
\item $FN$ is the number of True Positives: how many predictions were labeled as False but were actually True.
\end{enumerate}

So, the usual notion of accuracy is the proportion of the predictions from the model that were correct. This generalizes to multiclass classification problems (in which the target variable has a finite number of possible values) by considering the proportion of times that the model's prediction was correct. Regression problems are the ones in which the target variable has an infinite number of possible values but can be codified as a vector of numbers, for instance, the value of some building at two different times. We can access how accurate a regression model is in many ways, for instance the square difference between the prediction value and the real value, added through all training data points.

One important point to consider is that it is better to evaluate how accurate the system is with data different from the data used for training. This is because during the training phase the Machine Learning algorithm usually has the goal of providing the model that provides the best possible accuracy in the training data, among the possible models supported. If there is enough freedom among the possible models, then it might be possible to obtain a model that has a very good accuracy, but if we try to use this same model on other data, this same model performs very badly. For instance, if all functions from the training data to the output are allowed, then a model that simply memorizes the training data and outputs the correct result by looking at the memorized data and outputs a random answer if the input is not in the training data will have one hundred percent accuracy in the training data, but we can obviously provide no guarantee for data points not present. This problem is called \textbf{overfitting}, and is usually avoided by limiting how powerfull the model can be, in conjuction with verifying real accuracy values with data other than the training data, and we call this the \textbf{testing data}. Also, notice that to evaluate the model, the test data should also have the target value of each data point.

The classical goal of Machine Learning is to provide models with good \textit{test accuracy}, and an Machine Learning algorithm that produces models such that good results on the training data reflet on good results on testing data are said to \textbf{generalize} well. However, with the groth of Machine Learning applications in real-life scenarios and the impact on society as a whole, there has been a crescent focus on aspects other than simply maximizing the accuracy, in a similar way that we usually are worried only about how fast a cryptographic algorithm is but also about how safe it is.

\subsection{Fairness}

In the context of Machine Learning, fairness refers to the reduction, as much as possible, of \textbf{algorithmic bias}. Algorithm Bias is the bias introduced by algorithmic decisions. This bias might have a big social impact, as this can further existing unfair discrimination in society, as machine learning algorithms are being used to make more and more important decisions. One famouse example is the COMPAS recidivism algorithm, that has been used by the United States courts to estimate how likely someone is to reoffend in the future. It was revealed \cite{Compass} that this tool was heavily biased against black people. 

We will say that the result is \textbf{positive} for a data point if it benefits the person represented by that data point, and \textbf{negative} otherwise. We will say that the \textbf{unpriviledged group} is the group of people affected negatively by the bias, and the \textbf{priviledged group} is the other group of people.

Such biases can happen because of many factors. The algorithm itself might be introducing bias, or the data might be biased. The data might have been collected in a biased way (in the compass example, this would be the case if reincidivism data was collected more for black reincidivists than for white), or the data might be simply reinforcing some bias of the society. 

Also, the bias in society might be such that the data is in disagreement with reality (the unpriviledged group true values for the target variable would affect them in the same way as the priviledged group), or it is in agreement with reality because of structural biases in society. For instance, if the prediction of the algorithm is whether or not someone will have good grades if accepted to some university, people in the unpriviledged group might not have had as good oportunities in life as people in the priviledged group, so the data is correct when it says that those people will have worse grades. Even though, the results might still be considered unfair: this depends on the notion of fairness we consider. All of these unfairness possibilities can be futther divided into other types of unfairness, as was done in \cite{A Survey on Bias and Fairness in Machine Learning}.

Besides deliberate bias in the algorithm, such that the results of the algorithm do not reflect the data, and biased data, it is also possible to introduce bias because the algorithm might prioritize making correct predictions for the majority of the population, if it can't make correct predictions for both the majority and the minority. Another possibility is that the prediction might depend on past decisions of the algorithm, and we only know the result if the result provided is positive (for instance, we only know if someone will reincide if we release them). In this type of scenario, according to Learning Theory it's important to take suboptimal decisions to \textit{explore} different options and gather more data \cite{The Frontiers of Fairness in Machine Learning}, which might be considered unethical as it might have a big cost to society (releasing someone that's probably going to commit more crimes) or to the individual (not giving a life-saving drug to some patients as an experiment to see the survival rates for that specific group). 

Many different notions of algorithmic fairness have been developed, and some are not compatible \cite{uncompatibleFair}. Initially, the notions of fairness could be grouped into two main types: statistical and individual definitions of fairness\cite{The Frontiers of Fairness in Machine Learning}. Statistical (group) notions of fairness require some statistical metric to be similar for certain demographic groups, and individual notions enforce constraints on pairs of individuals, for instance requiring similar individuals to be treated similarly. Many problems with statistical notions and why they, in general, don't provide good individual guarantees are presented in \cite{awareness}\cite{Preventing Fairness Gerrymandering:Auditing and Learning for Subgroup Fairness}. Some of these problems include: satisfying the constraints for two protected attributes individually but not to combinations of these attributes, . One problem with both individual and group notions is \textit{composition}: it is not always the case that satisfying fairness constraints in individual, isolated, components of a system imply that fairness constraints will be satisfied for the whole system \cite{Fairness Under Composition}. Finally, there are also causal approaches to fairness notions, which we will discuss more in Section \ref{sec:theoRef}.

The techniques developed to reduce unfairness in algorithmic decision making can be divided into \textit{pre-processing}, \textit{in-processing} and \textit{post-processing}. Pre-processing techniques modify the training data to remove biases present there. In-processing techniques modifies the learning algorithm iteself, for instance by changing the objective learning function to include not only accuracy but also adding to it some statistical fairness metric, or including some constraint that it has to satisfy. Post-processing techniques act after the model is trained to reduce the unfairness in the decisions made by such a model.

We summarize below the ways in which unfairness might be introduced:

\begin{enumerate}
\item Algorithm results does not reflect the data.
    \begin{enumerate}
    \item The algorithm might optimize for the majority only, achieving good overall accuracy even though it's mostly wrong for minorities. This can be considered a type of Aggregation Bias.
    \item Systematic errors in the algorithm, that leads to biased estimation.
    \end{enumerate}
\item The Data can be biased, not reflecting the reality.
    \begin{enumerate}
    \item We can have a structural biases in society, such that people in unpriviledged groups do not have the necessary oportunities, but if they were treated similary to the priviledged group by society, they would have similar results. For instance, an unpriviledged group that doesn't have good education oportunities will have worse scores on exams because of historical discrimination, and although just looking at whether someone is in this group could lead to a good accuracy, it might be only perpetuate current unfair biases in society.
    \item The bias can also be introduced in a way that people in the unpriviledged group were misclassified before the data was colected, for instance maybe capable people in an unpriviledged group usually don't get a job even though they are actually as capable as the unpriviledged group.
    \item Data collection doesn't reflect the reality: Measurement bias (for instance, COMPASS used friend/family arrests as a proxy for a risk score present in the dataset), Ommited Variable bias (this violates assumptions of some learning models, for instance linear regression models usually assumes error terms uncorrelated with the parameters considered in the regression), Representation/Sampling Bias (biased sampling lacking the diversity of the population), Simpson's Paradox (if we don't have data on a confounder, correlations might be spurious \cite{Causality}).
    \item If the data is collected on a group fundamentally distinct from the one where it will be used, for instance another population (Population Bias) or the same population but at another time (Temporal Bias), unfair bias might be introduced.
    \item Data that relies on people's opinion is prone to many biases: Social Bias (people do what others are doing), Self-Selection Bias (people think that everyone agrees with them), and many others.
    \end{enumerate}
\item Data might depend on the algorithm previous output: Presentation Bias (the user is presented to some selected advertisements, for instance), Ranking Bias (search engines ordering results in a biased way), Popularity Bias (more popular itens are shown more). This might strengthen biases through time. 
\item Finally, the circunstances can change through time, either by influence of the algorithmt itself or other factors, which can worsen the quality of algorithms previously considered to provide good results (Emergent Bias).
\end{enumerate}


\subsection{Privacy}

% Privacy references: https://dl.acm.org/doi/abs/10.1145/3436755 and https://ieeexplore.ieee.org/document/9433648

% WMLMPASAO: When Machine Learning Meets Privacy: A Survey And Outlook

Privacy is the quality of maintaining secret specific information to unauthorized parties. In the context of Machine Learning, a privacy-preserving algorithm is one that doesn't allow information considered private/sensitive to be obtained by unauthorized parties. This has been named \textit{private ML}\cite{WMLMPASAO}, and the private data to be protected can be the data using to train the model or the model parameters and structure itself. It is also possible to use Machine Learning to enhance privacy, or to serve as an attack tool. We focus on \textit{private ML}. 

We call \textbf{adversary} the agent that wants to discover the private information, and \textbf{secret} the private information itself. There are some possible goals of the adversary, she might wish to recover the model itself (Model Extraction Attack) by trying to approximate the function that represents the model, to recover some feature or statistical property of the dataset (Feature Estimation Attack), to discover whether some individual data point is present in the dataset (Membership Inference Attack), or even recover the exact values of individual samples in the datset (Model Memorization Attack). We distinguish between the \textbf{White-Box access} and \textbf{Black-Box access} scenarios as the situation in which the adversary has or does not have full access to the trained model and their parameters, respectively.

One approach to improving privacy in Machine Learning is by encrypting the data or the model in a way such that the computations can be done with the encrypted data/model, and just the result is decrypted. Secure Multi-Party Computation is also an option if there are multiple parties responsible for this computation. Other option is to obfuscate the data or the model, by introducing pertubation in a way that protects the private information in a way that (hopefully) doesn't worsen much the results. There is also the aggregation approach, that focuses on multy-party computations such that the data of one party remains private to other parties, and is used by aggregation learning, for instance. Finally, it's possible to develop \textit{back-door} attacks: malicious modifications to the model might allow an adversary to infer private information, if she provides the specific inputs that are designed to trigger that. All of these are listed in more detail in \cite{WMLMPASAO}.


\subsection{Interpretability}



\subsection{Causality}



\subsection{Quantitative Information Flow}



