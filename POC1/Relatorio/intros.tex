\subsection{Machine Learning and scenario considered}


Machine Learning is the field of study that focuses on developing methods of learning general patterns from limited data. In recent years, important advances have been observed in Machine Learning research, and also the popularity and applications of some methods increased significantly. One example is the improvement of Convolutional Neural Network architectures, and the popularity of Generative Models. Also, some recent research is focused on the theory behind such machine learning methods\cite{SAAMAP}\cite{grohs2022mathematical}, and statistical learning in general \cite{Vapnik}. Part of the goal of this formalization is to provide more qualitative guarantees in regard not only to accuracy, but also fairness, privacy, interpretability, and other important qualities. We discuss these different goals in the next four subsections.

{\color{orange} Compensa mesmo criar ambiente de definição pra noções gerais tipo supervised nearling, algoritmo de ML, etc.? Eu não defini formalmente o que seria cada uma dessas coisas, deveria?}

In general, we consider \emph{supervised learning} problems: in this scenario, a \emph{machine learning algorithm} is an algorithm that receives many data points, which we call \emph{training data}, and outputs a \emph{model}. This model is itself an algorithm that receives a data point with some information omitted, encoded in what we call the \emph{target variable}, and outputs a guess of the omitted information, which we call the \emph{model prediction}. The model is then evaluated with other data points, ideally not the same ones used for training the model. This is called supervised learning because the algorithm has access to the target variable during the training process, which is not the case for unsupervised learning.

Figure \ref{fig:EEAAO} shows how the other concepts are related to machine learning in this context: we usually can assume that the training data is generated by some causal process, which can be modeled by a causal model; possible privacy attacks include performing sensitive information inference on the training data and on the model itself; we usually measure how accurate and fair a system is by analyzing its predictions for many data points; we can also obtain local and global explanations for complex models by this type of analysis.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{EverythingEAAO}
\caption{Figure representing the supervised learning scenario we consider throughout this work.{\color{orange} Aqui não deve fazer diferença, mas num artigo de verdade pode copiar e colar várias imagens do Google Imagens assim pra passar a ideia???}}\label{fig:EEAAO}
\end{figure}

\subsection{Accuracy}

\emph{Accuracy} is the notion of how close some estimate is to the true value we are estimating. In the context of Machine Learning, it represents how close the predictions of a given model are to the real value of the variable the model aims to predict. For binary classification (the scenario in which the target value has only two possible values), accuracy is defined in machine learning as $\frac{TP+TN}{TP+TN+FP+FN}$, where:

\begin{enumerate}
\item $TP$ is the number of True Positives: how many predictions were labeled as True and were really True.
\item $TN$ is the number of True Positives: how many predictions were labeled as True but were actually False.
\item $FP$ is the number of True Positives: how many predictions were labeled as False and were really False.
\item $FN$ is the number of True Positives: how many predictions were labeled as False but were actually True.
\end{enumerate}

So, the usual notion of accuracy is the proportion of the predictions from the model that were correct for the available data. This generalizes to multiclass classification problems (in which the target variable has a finite number of possible values) by considering the proportion of times that the model's prediction was the correct class. \emph{Regression} problems are the ones in which the target variable has an infinite number of possible values but can be codified as a vector of numbers: for instance, the value of some building at two different times. We can access how accurate a regression model is in many ways, for instance the square difference between the prediction value and the real value, summed for all training data points.

\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{polymOverfitEmpty}
\caption{Figure representing data points.}\label{fig:polymOverfitEmpty}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{polymOverfit}
\caption{Figure representing overfitted data.}\label{fig:polymOverfit}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{polymNotOverfit}
\caption{Figure representing non overfitted data.}\label{fig:polymNotOverfit}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{GeneralizationTreesWikipedia}
\caption{Figure representing the concept of generalization using trees. Obtained from Wikipedia.}\label{fig:GeneralizationTreesWikipedia}
\end{subfigure}
\caption{Figures representing the goal of generalization and the problem of overfitting.}
\end{figure}

Note that during the training phase the Machine Learning algorithm usually has the goal of providing the model that provides the best possible accuracy in the training data, among the possible models supported. If there is enough freedom among the possible models outputed from such algorithm, then it might be possible to obtain a model that has a very good accuracy, but if we try to use this same model on data other than the training data, this same model performs very badly. For instance, if we try to fit the data presented in \ref{fig:polymOverfitEmpty} with an arbitrary degree polynom, then it's possible to fit the data perfectly, as shown in \ref{fig:polymOverfit}. In this situation, the model obtained by the second-degree polynomial presented in \ref{fig:polymNotOverfit} would, in general, be more useful, even though it's accuracy on the training data is not $100\%$ as in \ref{fig:polymOverfit}. This problem is called \emph{overfitting}, and is usually avoided by limiting how powerful the model can be, in conjuction with verifying accuracy values with data points not present in training dataset, which we call the \emph{testing data}. Also, notice that to evaluate the model, the test data should also have the target value of each data point.


The classical goal of Machine Learning is to provide models with good \emph{test accuracy}, the accuracy evaluated for the test data, and an Machine Learning algorithm that produces models such that good results on the training data reflet on good results on testing data are said to \emph{generalize} well. The idea is that the model captures a more general abstract concept than the simple memorization of the training data, as illustrated by figure \ref{fig:GeneralizationTreesWikipedia}. 

However, note that accuracy does not always represent how accurately the model fulfills all of the goals of the relevant stakeholders. This is because, with the growth of Machine Learning applications in real-life scenarios and the impact on society as a whole, there has been a crescent focus on aspects other than simply maximizing the accuracy. This is similar to way that we sometimes are worried not only about how safe a cryptographic algorithm is but also about how computationally efficient it is. 

One classical binary classification example presented in figure \ref{fig:TruePosWikipedia} is the concern with \emph{sensitivity} and \emph{specificity}: how much of the data points with target value equal to $1$ are predicted by the model to have target value $1$ and how much of the poins with target value $0$ are predicted to have value $1$ instead, respectively. One example in which this distinction is very important is in medical diagnosis: imagine that the model is predicting if someone has a deadly disease and a simple and low-risk treatment, a fase negative is worse for the patient than a false positive diagnosis of having the disease. We can then create machine learning models that optimize for something other than the accuracy, for instance a combination of sensitivity and specificity that better suits our needs. The function that we aim to optimize with our model is sometimes called the \emph{objective function}, and we can encode many different goals in these functions, with one classical example is adding a \emph{regularization term} that penalises more complex models, which sometimes helps in avoiding overfit.

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\textwidth]{TruePosWikipedia}
\caption{Figure representing the concepts of sensitivity and specificity. Obtained from Wikipedia. {\color{orange} Precisa falar que foi obtido da wikipedia?}}\label{fig:TruePosWikipedia}
\end{figure}

Usually, there are also other important goals to keep in mind when developing a machine learning algorithm: the fairness goal aims to reduce unfair disparities in treatment of different individuals by machine learning models, the privacy goal aims to reduce the chance of someone discovering confidential or private data without authorization, and the explainability goal aims to improve the comprehension of how complex models work. Causality and Quantitative Information Flows are not goals by themselves, but methods for the representation and pursuit of other goals.

\subsection{Fairness}

In the context of Machine Learning, fairness refers to the reduction, as much as possible, of \emph{algorithmic bias}, the bias introduced by algorithmic decisions. This bias might have a big social impact because this can further existing unfair discrimination in society, as machine learning algorithms are being used to make more and more important decisions. One famous example is the COMPAS recidivism algorithm, that has been used by the United States courts to estimate how likely someone is to reoffend in the future. It was revealed \cite{Compass} that this tool was heavily biased against black people. 

For binary classification will say that the result is \emph{positive} for a data point if it benefits the person represented by that data point, and \emph{negative} otherwise. We will say that the \emph{unpriviledged group} is the group of people affected negatively by the bias, and the \emph{priviledged group} is the other group of people.

Such biases can happen because of many factors. The algorithm itself might be introducing bias, or the data might be biased. The data may have been collected in a biased way (in the compass example, this would be the case if reincidivism data was collected more for black reincidivists than for white), or the data might be simply reinforcing some bias in the society. 

Also, the bias in society might be such that the data is in disagreement with reality (the unpriviledged group true values for the target variable would affect them in the same way as the priviledged group), or it is in agreement with reality because of structural biases in society. For instance, if the prediction of the algorithm is whether or not someone will have good grades if accepted to some university, people in the unpriviledged group might not have had as good oportunities in life as people in the priviledged group, so the data is correct when it says that those people will have worse grades. Even though, the results might still be considered unfair: this depends on the notion of fairness we consider. All of these unfairness possibilities can be futther divided into other types of unfairness, as was done in \cite{mehrabi2021survey}. Image \ref{fig:whereUnfair} illustrates where unfairness might come from, and we summarize below the ways in which unfairness might be introduced:

\begin{enumerate}
\item Algorithm results does not reflect the data.
    \begin{enumerate}
    \item The algorithm might optimize for the majority only, achieving good overall accuracy even though it's mostly wrong for minorities. This can be considered a type of Aggregation Bias.
    \item Systematic errors in the algorithm, that leads to biased estimation.
    \end{enumerate}
\item The Data can be biased, not reflecting the reality.
    \begin{enumerate}
    \item We can have a structural biases in society, such that people in unpriviledged groups do not have the necessary oportunities, but if they were treated similary to the priviledged group by society, they would have similar results. For instance, an unpriviledged group that doesn't have good education oportunities will have worse scores on exams because of historical discrimination, and although just looking at whether someone is in this group could lead to a good accuracy, it might be only perpetuate current unfair biases in society.
    \item The bias can also be introduced in a way that people in the unpriviledged group were misclassified before the data was colected, for instance maybe capable people in an unpriviledged group usually don't get a job even though they are actually as capable as the unpriviledged group.
    \item Data collection doesn't reflect the reality: Measurement bias (for instance, COMPASS used friend/family arrests as a proxy for a risk score present in the dataset), Ommited Variable bias (this violates assumptions of some learning models, for instance linear regression models usually assumes error terms uncorrelated with the parameters considered in the regression), Representation/Sampling Bias (biased sampling lacking the diversity of the population), Simpson's Paradox (if we don't have data on a confounder, correlations might be spurious \cite{Causality}).
    \item If the data is collected on a group fundamentally distinct from the one where it will be used, for instance another population (Population Bias) or the same population but at another time (Temporal Bias), unfair bias might be introduced.
    \item Data that relies on people's opinion is prone to many biases: Social Bias (people do what others are doing), Self-Selection Bias (people think that everyone agrees with them), and many others.
    \end{enumerate}
\item Data might depend on the algorithm previous output: Presentation Bias (the user is presented to some selected advertisements, for instance), Ranking Bias (search engines ordering results in a biased way), Popularity Bias (more popular itens are shown more). This might strengthen biases through time. 
\item Finally, the circunstances can change through time, either by influence of the algorithmt itself or other factors, which can worsen the quality of algorithms previously considered to provide good results (Emergent Bias).
\end{enumerate}



\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{whereUnfair}
\caption{Figure representing some of the main possible sources of unfairness.}\label{fig:whereUnfair}
\end{figure}

Besides deliberate bias in the algorithm, such that the results of the algorithm do not reflect the data, and biased data, it is also possible to introduce bias because the algorithm might prioritize making correct predictions for the majority of the population, if it can't make correct predictions for both the majority and the minority. Another possibility is that the prediction might depend on past decisions of the algorithm, and we only know the result if the result provided is positive (for instance, we only know if someone will reincide if we release them). In this type of scenario, according to Learning Theory it's important to take suboptimal decisions to \emph{explore} different options and gather more data \cite{chouldechova2018frontiers} (found in \url{arxiv.org}), which might be considered unethical as it might have a big cost to society (releasing someone that's probably going to commit more crimes) or to the individual (not giving a life-saving drug to some patients as an experiment to see the survival rates for that specific group). 

Many different notions of algorithmic fairness have been developed, and some are not compatible \cite{alves2023survey}. Initially, the notions of fairness could be grouped into two main types: statistical and individual definitions of fairness\cite{chouldechova2018frontiers}. Statistical (group) notions of fairness require some statistical metric to be similar for certain demographic groups, and individual notions enforce constraints on pairs of individuals, for instance requiring similar individuals to be treated similarly. Many problems with statistical notions and why they, in general, don't provide good individual guarantees are presented in \cite{Awareness}\cite{kearns2018preventing}, for instance one such problem is satisfying the constraints for two protected attributes individually but not to combinations of these attributes. One problem with both individual and group notions is \emph{composition}: it is not always the case that satisfying fairness constraints in individual, isolated, components of a system imply that fairness constraints will be satisfied for the whole system \cite{dwork2018fairness}. Finally, there are also causal approaches to fairness notions, which we will discuss more in Section \ref{sec:theoRef2}. In general, it is not possible to satisfy some of the main notions of fairness at the same time\cite{hellman2020measuring}\cite{bell2023possibility}\cite{zemel2013learning}, and which fairness notion to use depends a lot on the specific goals of each different system. We will now define some of the main notions of fairness. We consider that $Y$ is the binary target variable, with $Y=1$ as the positive result and $Y=0$ as the negative one; $A$ is the binary sensitive attribute, with $A=1$ for the priviledged group and $A=0$ for the unpriviledged group; $\hat Y$ is the model prediction of the target variable value; $X$ is a set of legitimate factors that can be used for classification.

\begin{definition}[Equal Opportunity Difference] We define \emph{Equal Opportunity Difference} as $P(\hat Y = 1| A = 1, Y =1) - P(\hat Y = 1| A = 0, Y = 1)$. Equal Opportunity is \emph{satisfied} if the Equal Opportunity Difference is equal to zero.
\end{definition}

\begin{definition}[Statistical Disparity] We define \emph{Statistical Disparity}, also known as \emph{Demographic Disparity}, as $P(\hat Y = 1| A = 1) - P(\hat Y = 1| A = 0)$. Statistical (Demographical) \emph{Parity} is \emph{satisfied} if the Statistical Disparity is equal to zero.
\end{definition}

\begin{definition}[Conditional Statistical Disparity] We define \emph{Conditional Statistical Disparity}, conditioned on $x$, as $P(\hat Y = 1| A = 1, X = x) - P(\hat Y = 1| A = 0, X = x)$. Conditional Statistical \emph{Parity} is \emph{satisfied} if the Conditional Statistical Disparity is equal to zero.
\end{definition}

A possible general definition for \emph{individual fairness} notions is that an algorithm is considered fair if it gives similar outcomes to similar individuals, according to similarity notions relevant for the specitic scenario considered.

The techniques developed to reduce unfairness in algorithmic decision making can be divided into \emph{pre-processing}, \emph{in-processing} and \emph{post-processing}. Pre-processing techniques modify the training data to remove biases present there. In-processing techniques modifies the learning algorithm iteself, for instance by changing the objective learning function to include not only accuracy but also adding to it some statistical fairness metric, or including some constraint that it has to satisfy. Post-processing techniques act after the model is trained to reduce the unfairness in the decisions made by such a model.

In general, just removing the variables that would be considered unfair to use directly to classify an individual is not enough to guarantee fairness. As illustrated in image \ref{fig:correlated}, the variables we would remove might be highly correlated to other variables, which could be used by the model to discriminate almost as if we hadn't removed any variable. Also, even if the machine learning model itself didn't use any sensitive variables or correlated attributes for the predictions, we still need to collect this sensitive data to be able to measure how unfair the model is.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{correlated}
\caption{Figure representing potential correlations between sensitive variables and other factors: if the disease status is a sensible information that could be used for unfair discrimination, then removing this information might not be enough to avoid unfair discrimination, as smoking, age and other factors combined might lead to unfair discrimination almost as if the model had direct access to the sensitive values.}\label{fig:correlated}
\end{figure}

\subsection{Privacy}

{\color{red} Privacy references: https://dl.acm.org/doi/abs/10.1145/3436755 and https://ieeexplore.ieee.org/document/9433648. The second one doesn't add much.

 WMLMPASAO: When Machine Learning Meets Privacy: A Survey And Outlook


Membership Inference Attacks Against Machine Learning Models

Privacy-preserving classification of customer data without loss of accuracy

CryptoDL: Deep Neural Networks over Encrypted Data

About dependence between individuals: Dependence Makes You Vulnerable: Differential Privacy Under Dependent Tuples


}
% Local differential privacy and its applications: A comprehensive survey 

{\color{red} Falar de DP e LDP também, são importantes: A Comprehensive Survey on Local Differential Privacy}

{\color{red} Falar de Federated Learning também!}

In the context of Machine Learning, a privacy-preserving algorithm is one that doesn't allow information considered private/sensitive to be obtained by unauthorized parties. This has been named \emph{private ML}\cite{liu2021machine}, and the private data to be protected can be the data using to train the model or the model parameters and structure itself. It is also possible to use Machine Learning to enhance privacy, or to serve as an attack tool. We focus on \emph{private ML}. 

We call \textbf{adversary} the agent that wants to discover the private information, and \textbf{secret} the private information itself. There are some possible goals of the adversary, she might wish to recover the model itself (Model Extraction Attack) by trying to approximate the function that represents the model, to recover some feature or statistical property of the dataset (Feature Estimation Attack), to discover whether some individual data point is present in the dataset (Membership Inference Attack), or even recover the exact values of individual samples in the datset (Model Memorization Attack). We distinguish between the \textbf{White-Box access} and \textbf{Black-Box access} scenarios as the situation in which the adversary has or does not have full access to the trained model and their parameters, respectively.

One approach to improving privacy in Machine Learning is by encrypting the data or the model in a way such that the computations can be done with the encrypted data/model, and just the result is decrypted. Secure Multi-Party Computation is also an option if there are multiple parties responsible for this computation. Other option is to obfuscate the data or the model, by introducing pertubation in a way that protects the private information in a way that (hopefully) doesn't worsen much the results. There is also the aggregation approach, that focuses on multy-party computations such that the data of one party remains private to other parties, and is used by aggregation learning, for instance. Finally, it's possible to develop \emph{back-door} attacks: malicious modifications to the model might allow an adversary to infer private information, if she provides the specific inputs that are designed to trigger that. All of these are listed in more detail in \cite{liu2021machine}.


\subsection{Explainability}

Explainability, loosely defined, concerns the ability to assign meaning to why some model took one or more decisions, in a way that can be interpreted by humans. There is currently no consensus for a precise definition of the term, and many papers argue about distinctions between interpretability and explainability. For instance, \cite{gilpin2018explaining} defines interpretability as the property of being able to describe the internal working of systems to humans and \emph{completeness} as the property of being able to accurately describe the operation of the system, and an explainable system has both properties at an acceptable level. The paper \cite{roscher2020explainable} draws this distinction in a different way: they define transparency as the higher-level explanation given by the designers of the system of their choices of architecture, algorithm and hyperparameters, while interpretations are defined as answers to the question \qm{what does the model bases its decision on?}, and explanations as the combination of interpretations and contextual information from domain knowledge. In general, it is considered necessary that an explanation both explains the inner workings of a system accurately and is comprehensible enough for humans with the relevant domain knowledge. There is a trade-off between these two goals, as we want not only to reach a balance between simplicity and accuracy, but we also want important biases in the model to be evident in the explanation, as mentioned in \cite{gilpin2018explaining}.

There are some classifications of the existing approaches to explainability, but we focus on the classification defined in \cite{linardatos2020explainable}. We can divide the approaches based on whether they are \textbf{local} or \textbf{global}: local explanations focus on explaining individual decisions, and global explanations focus on explaining the overall workings of the model. LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified} are examples of methods that provide local explanations and can be used to derive global explanations. We can also divide the approaches into \textbf{Model Agnostic} and \textbf{Model Specific}: the former refers to methods that don't depend on the inner workings of the model (for instance, SHAP \cite{lundberg2017unified} and LIME \cite{ribeiro2016should}), the latter refer to methods developed to work only for a specific group of models (for instance, Grad-CAM and Shap-CAM are specific for Convolutional Neural Networks). Finally, we can divide explainability approaches on the data types these methods deal with and the purpose of the explanations.

As discussed at \cite{belle2021principles}, explanations may be usefull to data scientists, business owners, model risk analysts, regulators and consummers, each with different goals in mind. The concerns that might be reduced by the use of explainable models include: correctness (only variables relevant should be used in the final decisions and we should not use spurious correlations incorrectly), robustness (the model should not be susceptible to small pertubations), bias (the model should not be biased against specific subgroups), improvement (we might want to improve the model, and explanations can aid in this goal), transferability (the model should be useful in populations other than the one used to train and test the data) and human comprehensibility (this can aid an expert or even a non-expert in using and even trusting the results provided by the model). The paper also mentions some criterias for evaluating explanations, that include how comprehensible the explanation is, how they accurately capture the models they aim to explain, how accurately they can be used to predict other outcomes of the model, how they scale to larger and more complex models, and how restrictive they are on the type of accepted model, some of these notions are further explored in \cite{carvalho2019machine}. They also evaluate explanations by example (for instance, counterfactuals \cite{verma2020counterfactual} that provide examples with small changes to an input that can modify the output), and explanations by simplification (approximating a complex model by a simpler one). These approaches are different from SHAP, for instance, as it is based on game-theoretic based feature importance concepts. LIME can be considered to provide explanations by simplification, by locally approximating the model to a linear model. As mentioned in \cite{roscher2020explainable}, explanations can also be used to enhance scientific research in natural sciences.

Some models are inheretely interpretable, as their inner working is easier to understand for humans. Some examples mentioned in \cite{belle2021principles} are: Linear Logistic Regression, Decision Trees, K-Nearest Neighbors (KNN), Rule-based learning, Generalized Additive Models (GAMs) and Bayesian networks. Some researchers criticise this claim for some of these models, for instance, \cite{lipton2018mythos} points out that linear models are not necessarily easily interpretable, as they sometimes rely on un-interpretable and heavily-engineered features.

Other criticisms to the current development of explainability approaches were made: for instance, \cite{krishnan2020against} argues that explainability is always a means to an end, and by requiring explanations to our models we may be significaltly restricting the space of possibilities of dealing with the problems we need to face. For instance, there are approaches to verify if a model is fair that do not rely on explanations of the inner workings of the model (and in this case, relying on explanations might lead to other problems, as mentioned in \cite{ExplainAll}). Another possible problem with explainability is that if we rely on humans opinions we might end up with methods that are \emph{persuasive}, instead of \emph{accurate}, as these two properties might not be fully aligned. 

An in-depth survey on machine learning explainability can be found at \cite{burkart2021survey}, with some definitions regarding ontology and the philosophical meaning of explanations and knowledge sharing. 

{\color{red} Esse paper parece discutir porque black-box pode ser aceitável em ML pra decisões médicas ainda por cima: Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability}

\subsection{Causality}

Most of our discussion of causality will be based on the work of Judea Pearl \cite{Causality}. There are many alternative notions of causality, but most of them are already discussed in \cite{Causality}. Judea Pearl divides causality into three levels, according to the type of question we want to answer. 

\textbf{The first level of causation} deals with questions that can be answered by looking at the data. For instance, questions that can be answered by Bayesian networks, deep neural networks or other machine learning algorithms. According to Pearl, this kind of question can be written in terms of what we see: for instance, given that we see that the floor of an entire street is wet then it probably rained just before. Although the examples usually involve some form of conditional probability, Pearl states that anything that can be computed from the joint distribution belongs to the first level of causation. This includes basically all of the usual machine learning approaches. The notation used by Pearl is the usual probabilistic notation, $P(Y=y|X=x)$, abreviated as $P(y|x)$, this means that we observed $X=x$ and want to know how likely it is for $Y$ to have the value $y$.

\textbf{The second level of causation} concerns questions of the type: \qm{what's the consequence of some specific action}. For instance: \qm{if we make a street wet by throwing water manually into it, what is the probability that it rained?}. The difference between seeing and doing is the fundamental distinction between Pearl's first and second levels of causation. Not only is the meaning completelly different, it's also impossible to answer questions at the second level of causation by only looking at the data: extra assumptions are necessary. In our example, if we look only at the data what we see is a strong correlation between raining and the floor getting wet, and in principle we have no idea which one causes which. There are also scenarios in which two variables are strongly correlated but neither one causes the other. For instance, in some places the number of ice cream sales is strongly correlated with the number of deaths by  shark attacks, but clearly that's because the times of the year when more people go to the beach are the same of when people buy more ice creams. Pearl proclaims that any method of solving questions at the second level of causation must rely on assumptions beyond the data. One way of structuring many of the relevant assumptions is by using a directed acyclic graph in which each vertex is a variable and each edge represents the causal directions between two variables. There are many other types of assumptions that can be made, Pearl discusses them in detail at \cite{Causality}. The notation used by Pearl is $P(Y=y|do(X=x))$, which can be abreviated as $P(y|do(x))$ or $P(y|\hat{x})$, which can be interpreted as how likely $Y$ is to have value $y$ when we set the value of $X$ to $x$ manually, and when we say \qm{manually}, we mean by an external intervention, that is not causally affected by the variables in the system. 

\textbf{The third level of causation} regards questions of the type: \qm{what would have happened had something been different?}. For instance, \qm{what would be the average temperature of Earth had the Industrial Revolution never happened?}. Note that we make an observation about something that happened, then we think what would have happened if we changed something that already happened. This is what Pearl calls a counterfactual question: it requires imagining alternative worlds. To deal with this kind of question, Pearl proposes \textbf{Structural Causal Models} (SCMs), which consider deterministic relationships between variables and adds all the uncertainty to the values of unobserved variables. Also, in the way the Pearl defines counterfactuals, there is a fundamental distinction between counterfactuals and actions with observations: in his notation, when we write the probability of some variable reaching some value given that there was an action and an observation, we interpret this as if the observation came \emph{after} the action. This is different from the counterfactual notation, in which the observation comes before the action. The notation used by Pearl for the counterfactual notion is $P(Y_{x'} = y| X=x)$, which can be interpreted as how likely it is for $Y$ to have value $y$ in the world that we manually set $X$ to $x'$, given that we observed that the value of $X$ is $x$. This is different from $P(Y=y|do(X=x'),X=x)$, as this should be interpreted as the probability that $Y$ has a value $y$ when we manually set $X$ to $x'$ and then observe that $X$ has value $x$, which doesn't make sense if $x \neq x'$ (we shouldn't be able to condition on impossible scenarios). This difference of interpretation can lead to confusion, and was mentioned in the second part of the \qm{question to author} at subsection $11.7.2$ of \cite{Causality}.

In general, Judea Pearl's approach to causality assumes an Directed \emph{Acyclic} Graph (DAG) for the relations between variables. This is what is called \textbf{recursive models} in \cite{Causality}. Many results apply only to this type of models, but some generalizations are available and mentioned in \cite{Causality}. Non-recursive models can be used for representing feedback loops: for instance, price and demand in economic models. This type of model has been widely used in economics in the form of Structural Equations, as presented in subsection $1.4.1$ of \cite{Causality}. Pearl also argues extensively about the possible causal interpretation of such equations, and defines Structural Causal Models as modens in which each variable has an equation that determines its value. This type of model can be used for answering questions of the third level of causation.

Pearl presents many results about \textbf{Causal Discovery} on the second chapter of \cite{Causality}. Many theoretical results are presented of what can and can not be done regarding the discovery of causal relations when we have access only to data. If two causal structures are capable of generating the same joint distributions, then we will be unable to distinguish between them if we observe only data. Also, sometimes a distribution is unstable for some model, in the sense that although the model can generate this distribution, it can only do so for some very specific configuration of the parameters. For instance, if we have $A$ and $B$ as the outcome of two independent fair coin throws ($1$ if heads and $0$ otherwise, for instance), and $C$ as the XOR between them. In the resulting joint distribution of the three variables, each pair of variables will be marginalyl independent but dependent if we condition on the third variable. This can be generated by three different causal structures, but only one is stable to small changes in the model (for instance, to small changes in the probability of each coin). With the assumptions that the observed distribution is stable in respect to the underlying causal model, and based on the principle of Occam's Razor, Pearl proceeds to define algorithms to recover as much information as is possible with only the data. In general, it is impossible to distinguish some relations: for instance, $A \rightarrow B$ (meaning that $A$ causes $B$) is indistinguishable from $A \leftarrow B$ or $A \leftarrow U \rightarrow B$ for an unobservable $U$, as all three of them can generate exactly the same distributions on $A,B$ in a stable way, depending on the model's parameters. Notice that $A \leftarrow U \rightarrow B$ can also generate distributions in which $A$ and $B$ are independent: we can, for instance, set $U$ as the result of a fair four-sided dice with values $\{0,1,2,3\}$, $X$ as an indicator variable of the parity of the result ($1$ if it is odd and $0$ if it is pair) and $Y$ as an indicator of whether the result is bigger than $1$ ($0$ if it is not, $1$ if it is), in this case $X$ will be independent of $Y$ even though there is an unobservable confounder $U$. But this is not stable, in our example if we change slightly the probability distribution of $U$, for instance by increase the probability of the outcome $0$, then $X$ and $Y$ become dependent, $P(X=0|Y=0) \neq P(X=0)$. Pearl provides an extensive analysis of stability and how causal relations are reflected as statistical dependencies between variables in the data.

\textbf{Causal Inference} regards the problem of infering attributes of a causal model given it's strctured. In this situation, we have some causal model a priori, and want to estimate quantities such as the $P(Y=y|do(X=x))$. Pearl introduces the \textbf{$do$ calculus}, which provides a way of deriving expressions for such quantities that do not depend on a do operator, and can thus be estimated from data. The $do$ calculus is based on three basic inference rules, which were shown to be necessary and suficient, in the sense that a causal effect based on the $do$ operator is identifiable by observing only the data and the assumption of the DAG structure of the underlying model if and only if we can derive an expression without the $do$ operator using only these three rules. Many other quantities can be defined with Pearl's framework, for instance there are also definitions for the identifying the results of dynamic plans, in which one action comes after others and might depend on the results of previous actions. This is further discussed in section $4.4$ of \cite{Causality}. In section $4.5$, Pearl dives into two other causal concepts: \textbf{Direct and Indirect Effects}, which regard the effects that some variable $X$ have on other variable $Y$ that do or do not depend on other variables. For instance, smoking causes tar deposits in the lungs and also cancer, but we can think of how much of the effect of smoking on the probability of cancer is due to tar deposits, and how much is due to other more direc factors. Natural Direct Effects are \qm{average} estimations of direct effects for the different values the variables can assume, as the way that Pearl defines Direct and Indirect Effects is dependent of the exact values of the variables in question.

Another contribution of the causality framework presented in \cite{Causality} is the causal approach to \textbf{confounding}. In some situations, it is necessary to condition on some variables to account for possible spurious correlations that might arrise due to confounding, but in other situations controling for some variables would actually create \emph{new} spurious correlations. This also depends on what we want to compute, but if the goal is computing the effect of some intervention, then the $do$-calculus can be used. Pearl argues in chapter $6$ of \cite{Causality} in favor of the causal approaches to confounding.

\textbf{Counterfactual statements} are defined in terms of simpler axioms, \cite[Chapter~7]{Causality} further discusses many results about counterfactuals. These results include how to use an Structural Causal Model to estimate the value counterfactual statements, or how to check if this is even possible to estimate only with the SCM and data. Chapter $8$ delves into the problem of bounding values of expressions that can not be computed directly. Pearl also discusses how some extra assumptions and tools can help in the estimation of expressions with the $do$ operator and counterfactual expressions, for instance, if we can experimentally change the value of some variables via external interventions then some effects are easier to estimate.

Finally, there are some subtleties to the meaning of causation in different settings. Pearl discusses many notions of causation, including the probability of necessity, the probability of sufficiency, the natural direct effect (which we already mentined), the actual cause, and others. We say that $X=1$ was a sufficient cause of $Y=1$ if when we are in a situation in which $X=0$ and $Y=0$ we expect that manipulating the value of $X$ so it becames $1$ is likely to change the value of $Y$ to $1$, in Pearl's notation $P(Y_{X=1}=1 | X=0, Y=0) \approx 1$, and $P(Y_{X=1}=1 | X=0, Y=0)$ is called the \textbf{Probability of Sufficiency}. We say that $X=1$ was a necessary cause of $Y=1$ if when we are in a situation in which $X=1$ and $Y=1$ we expect that manipulating the value of $X$ so it becomse $0$ is likely to change the value of $Y$ to $0$, in Pearl's notation $P(Y_{X=0}=0 | X = 1, Y = 1) \approx 1$, and $P(Y_{X=0}=0 | X = 1, Y = 1)$ is called the \textbf{Probability of Necessity}. There is also the \textbf{Probability of Necessity and Sufficiency}, defined as the chance that manipulating the value of $X$ so it becomes $1$ will set $Y$ to $1$ and manipulating $X$ so it becomes $0$ will set $Y$ to $0$, written as $P(Y_{X=1}=1,Y_{X=0}=0)$. Pearl also defines the \textbf{Probability of Disablement} as $P(Y_{X=0}=0|Y=1)$ and the \textbf{Probability of Enablement} as $P(Y_{X=1}=1|Y=0)$. Many relations between these values, bounds and conditions for identification are presented in \cite[Chapter~9]{Causality}.

The notion of Actual Cause is defined as an alternative to the sufficient and necessary notions of causation. Pearl mentions that necessary causation is closer to \textbf{token-level}, more individual than generic, as it conditions on events that really happenned, while sufficient causation is closer to \textbf{type-level}, more generic than individual, as the events we condition on are less specific and related to an alternative imaginary scenario. The Actual Cause is intended to be token-level, to define what actually caused something. It is defined in terms of Causal Beams and Sustenance. 

We say that $X=x$ \textbf{causally sustains} $Y=y$ in $U=u$ (representing the uncertainty, the unobservable factors) relative to contingencies in $W$ if and only if we have: $X=x$ and $Y=y$ under $U=u$, for any $w$ we get $Y=y$ under $U=u$ and interventions that set $X=x$ and $W=w$, and we get $Y=y' \neq y$ under $U=u$ and interventions that set $X=x',W=w'$ for some $x' \neq x$ and some $w'$. In other words, $X=x$ causally sustains $Y=y$ in the circunstances $U=u$ relative to $W$ when $X=x$ and $Y=y$ in the scenario $U=u$, the value of $Y$ never changes if we change the value of $W$ and keep the value of $X$, and the value of $Y$ can change if we change the values of $X$ and $Y$, keeping everyting else as it is with $U=u$. This means that in the situation that actually happenned $U=u$, then $X=x$ is enough to \emph{sustain} $Y=y$ under inverventions on $W$, but if we intervene to change $X$ there will be a scenario in which intervening in $W$ changes $Y$. If $W=\emptyset$, then $X=x$ causally sustains $Y=y$ in $U=u$ relative to $W$ if in $U=u$ we have $X=x$ and $Y=y$, but it is possible to change $Y$ by changing $X$.

A \textbf{Causal Beam} is a causal model defined in terms of circunstances $U=u$ and another causal model, such that the parents of each node in the new model are sufficient to entain the value of the node regardless of the value of changes on the other parent's values, and that it is possible to change the value of other parents and the new parents to change the value of the node. If changing the value of only the new parents is enough to change the value of the node for every node, then the Causal Beam is considered a \textbf{Natural Beam}. Natural Beams represent the simplified version of the model that represents the actual scenario $U=u$, such that the parents of each node are enough to sustain the value of the node regardless of changes in other variables, and are also capable of changing the value of the node by themselves. Pearl notes that in the definition of Causal Models provided, the parents of a node are defined in a way that makes the functions of the model non-trivial regarding all their arguments and all possible circunstances $u$, but when we consider a specific value of $U$ we can simplify the model further. The example introduced by Pearl considers $f_i(x_1,x_2,u) = ax_1+bux_2$ as the function that defines the value of variable $V_i$ in the original model: in this scenario when $u=0$ the value of $x_2$ becomes irrelevant, so we can simplify the model by defining $f_i(x_1) = ax_1$. 

Finally, we say that $X=x$ is an \textbf{Actual Cause} of $Y=y$ in the state $U=u$ if and only if there is a natural beam under circunstances $U=u$ such that if intervene with $X=x$ then $Y=y$ and if we intervene with $X=x'$ for some $x'\neq x$ we get $Y \neq y$. This represents a token-level causation, whether or not $X=x$ actually caused $Y=y$ in the real scenario $U=u$. As with many of the results presented, these definitions assume we have a full description of the causal model. The notion of Actual Cause is further discussed in Chapter $10$ of \cite{Causality}.


\subsection{Quantitative Information Flow}

The area of Quantitative Information Flow deals with methods of measuring information leak from systems. This estimation is important to consider when developing real systems, as some information leaks are acceptable. For instance, whenever someone tries to authenticate with an username and password, but incorrectly guesses the password, some information leaked about the real value of the password: we now at least know that it's not the one that was tried. But intuitively, this is acceptable, while revealing the real password whenever someone makes an incorrect guess is unacceptable. How to adequately quantify the ammount of information leak from a system might depend on the goals of the people involved and on the information they have before the system executes.

We define an \textbf{adversary} as an agent that tries to gain something with the information that leaks from the system, the \textbf{secret} as the data that the system processes, the \textbf{prior} as the distribution on secrets that represents the knowledge of the adversary before the system is run, and the \textbf{posterior} as the hyper-distribution (a distribution on distributions) on secrets that represents the knowledge of the adversary after the system is run. We use an information-theoretic \textbf{channel} to represent the distributions of observable values outputed from the system, which might depend on the secret value. The set $\mathcal{X}$ represents the set of possible values of the secrets, the set $\mathcal{Y}$ represents the possible values of observable outputs of the system, and the set $\mathcal{W}$ represents the possible values of actions the adversary might take.

We consider the $g$-vulnerability framweork, introduced in \cite{QIF}. We define a gain function $g : \mathcal{W} \times \mathcal{X} \rightarrow \mathbb{R}$, such that $g(w,x)$ defines the gain of the adversary if she takes the action $w$ when the actual secret value is $x$. We consider a zero-sum game: the gain of the adversary is exactly the loss of the people responsible for the system. The \textbf{prior vulnerability} of the system is defined as the average gain of the adversary if she takes the action that maximizes her gain, according to the prior distribution on secrets that represents her knowledge of the secret. The \textbf{posterior vulnerability} of the system is defined in the same way, but considering the hyper-distribution that represents the posterior knowledge. Notice that each distribution in the posterior hyper-distribution represents the knowledge of the adversary after some of the possible observations after the system is run, so it represents what we, before the system runs, estimate will be the future values of the adversarial knowledge after the system runs. The \textbf{additive leakage} can be defined as the difference between posterior and prior vulnerability, and the \textbf{multiplicative leakage} as the result of dividing of the posterior vulnerability by the prior vulnerability values. As teorema leak >= 1 >= 0

There are some valuable theoretical results about channels regarding the relationships between prior and posterior $g$-vulnerabilities. Chapter $7$ of \cite{QIF} shows results about the \emph{capacity} of a channel, which is the maximum possible (aditive or multiplicative) leakage that can happen through a channel if we fix either the prior, the gain function or neither. Chapter $9$ discusses results about \emph{refinement} of channels: in short, a channel is strictly better (for all priors and gain functions) to another channel in respect to the posterior vulnerability if and only if it can be written as a post-processing of this other channel. Chapter $10$ discusses the notion of \emph{Dalenious vulnerability}: it might be the case that the adversary is interested in a secret other than the one considered in the system, and can obtain information about this other system via a known joint distribution between this other secret and the secret that the system considers. In this case, a channel is also strictly better than another in respect to Dalenious leakage, for any such joint distribution and gain function, if and only it can be written as a post-processing of this other channel. Chapter 11 \cite{QIF} discusses the axiomatic characterization of the notion of vulnerability, and even how some results can be obtained by different axioms that consider the worst-case scenario instead of the average gains of the adversary.


%{\color{red} Maybe mention the distinction between prior and posterior knowledge, the use of hyper distributions, abstract channels and maybe even Markovs and HMMs.}
