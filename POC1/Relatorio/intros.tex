\subsection{Machine Learning}

Machine Learning is the field of study that focuses on developing methods of learning patterns from data in a way that generalizes to situations not present in the data. In recent years, significative advances have been observed in Machine Learning research, and also the popularity and applications of some methods increased significantly. One example is the improvement of Neural Network Architectures\cite{?}, and the popularity of Generative Models\cite{?}. Also, some recent research is focused on the theory behind such machine learning methods\cite{livros de SAAMAP}, and statistical learning in general \cite{VladmirNaumovichVapnik}. Part of the goal of the formalization of such theory is to provide more qualitative guarantees, for instance, in regard not only to accuracy, but also fairness, privacy and interpretability. We discuss these different goals in the next four subsections.

In general, we consider \textbf{supervised learning} problems: a machine learning model is an algorithm that receives many data points, which we call \textbf{training data}, and outputs a model. This model is itself an algorithm that receives a data point without the target value and outputs a prediction of the target value, which represents one or more variable of interest. This is callled supervised learning because the algorithm has acces to the target variable during the training process.

\subsection{Accuracy}

Accuracy is the notion of how close some estimate is to the true value we are estimating. In the context of Machine Learning, it represents how close the predictions of a given model are to the real value of the variable the model aims to predict. For binary classification (the scenario in which the target value has only two possible values), accuracy is defined in machine learning as $\frac{TP+TN}{TP+TN+FP+FN}$, where:

\begin{enumerate}
\item $TP$ is the number of True Positives: how many predictions were labeled as True and were really True.
\item $TN$ is the number of True Positives: how many predictions were labeled as True but were actually False.
\item $FP$ is the number of True Positives: how many predictions were labeled as False and were really False.
\item $FN$ is the number of True Positives: how many predictions were labeled as False but were actually True.
\end{enumerate}

So, the usual notion of accuracy is the proportion of the predictions from the model that were correct. This generalizes to multiclass classification problems (in which the target variable has a finite number of possible values) by considering the proportion of times that the model's prediction was correct. Regression problems are the ones in which the target variable has an infinite number of possible values but can be codified as a vector of numbers, for instance, the value of some building at two different times. We can access how accurate a regression model is in many ways, for instance the square difference between the prediction value and the real value, added through all training data points.

One important point to consider is that it is better to evaluate how accurate the system is with data different from the data used for training. This is because during the training phase the Machine Learning algorithm usually has the goal of providing the model that provides the best possible accuracy in the training data, among the possible models supported. If there is enough freedom among the possible models, then it might be possible to obtain a model that has a very good accuracy, but if we try to use this same model on other data, this same model performs very badly. For instance, if all functions from the training data to the output are allowed, then a model that simply memorizes the training data and outputs the correct result by looking at the memorized data and outputs a random answer if the input is not in the training data will have one hundred percent accuracy in the training data, but we can obviously provide no guarantee for data points not present. This problem is called \textbf{overfitting}, and is usually avoided by limiting how powerfull the model can be, in conjuction with verifying real accuracy values with data other than the training data, and we call this the \textbf{testing data}. Also, notice that to evaluate the model, the test data should also have the target value of each data point.

The classical goal of Machine Learning is to provide models with good \textit{test accuracy}, and an Machine Learning algorithm that produces models such that good results on the training data reflet on good results on testing data are said to \textbf{generalize} well. However, with the groth of Machine Learning applications in real-life scenarios and the impact on society as a whole, there has been a crescent focus on aspects other than simply maximizing the accuracy, in a similar way that we usually are worried only about how fast a cryptographic algorithm is but also about how safe it is.

\subsection{Fairness}

In the context of Machine Learning, fairness refers to the reduction, as much as possible, of \textbf{algorithmic bias}. Algorithm Bias is the bias introduced by algorithmic decisions. This bias might have a big social impact, as this can further existing unfair discrimination in society, as machine learning algorithms are being used to make more and more important decisions. One famouse example is the COMPAS recidivism algorithm, that has been used by the United States courts to estimate how likely someone is to reoffend in the future. It was revealed \cite{Compass} that this tool was heavily biased against black people. 

We will say that the result is \textbf{positive} for a data point if it benefits the person represented by that data point, and \textbf{negative} otherwise. We will say that the \textbf{unpriviledged group} is the group of people affected negatively by the bias, and the \textbf{priviledged group} is the other group of people.

Such biases can happen because of many factors. The algorithm itself might be introducing bias, or the data might be biased. The data might have been collected in a biased way (in the compass example, this would be the case if reincidivism data was collected more for black reincidivists than for white), or the data might be simply reinforcing some bias of the society. 

Also, the bias in society might be such that the data is in disagreement with reality (the unpriviledged group true values for the target variable would affect them in the same way as the priviledged group), or it is in agreement with reality because of structural biases in society. For instance, if the prediction of the algorithm is whether or not someone will have good grades if accepted to some university, people in the unpriviledged group might not have had as good oportunities in life as people in the priviledged group, so the data is correct when it says that those people will have worse grades. Even though, the results might still be considered unfair: this depends on the notion of fairness we consider.

Many different notions of algorithmic fairness have been developed, and some are not compatible \cite{uncompatibleFair}.

%Even then, there are many results, some general enough to include the most commonly used fairness notions, indicate that fairness and accuracy can be fundamentally incompatible\cite{omnifair}\cite{Microsoft}\cite{carlos}. 

\subsection{Privacy}



\subsection{Interpretability}



\subsection{Causality}



\subsection{Quantitative Information Flow}



