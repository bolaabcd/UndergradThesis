\subsection{Accuracy $\times$ Fairness}

There are results that indicate an inherent trade-off between fairness and accuracy in machine learning:

\begin{enumerate}
\item \cite{Carlos} shows that there are trade-offs between Equal Oportunity Difference and accuracy such that, depending on the data distribution, it might impossible to achieve both perfect Equal Oportunity Difference and non-trivial accuracy. It also shows some other theoretical results that associate EOD and accuracy, such as sufficient conditions for the existance of non-trivially accurate predictors that lead to zero Equal Opportunity Difference and non-trivial accuracy and algebraic and geometric properties of the feasible values of Equal Opportunity Difference and accuracy.
\item \cite{Reductions} provides methods for computing the best possible accuracy given some level of fairness, for a general notion of fairness that encompasses many common metrics. They devise an algorithm for solving a constrained linear optimization problem that minimizes the error subject to fairness constraints, and provide experimental results for some datasets, including the COMPAS \cite{Compass} dataset. One interesting results is that for some datasets (such as the Compas dataset), it is possible to reduce Equal Opportunity Difference without changing much the accuracy, but for some datasets (such as the Dutch Census Dataset\cite{Dutch} with gender as the protected attribute and the goal is if someone has a prestigious occupation) {\color{red} Dutch Census deve ser esse: \url{https://www.researchgate.net/profile/Eric-Schulte-Nordholt/publication/286099390_The_Dutch_Census_2011/links/58a3228b458515d15fd942d2/The-Dutch-Census-2011.pdf}}
\item \cite{On the Impossibility of Fairness-Aware Learning from Corrupted Data} shows that it is always possible for an adversary to corrupt the data available for a learning algorithm in a way that the algorithm is unfair, and sometimes it is possible to do so without changing the accuracy.
\item \cite{How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness} Provides a method of finding the full Paretto front of accuracy versus fairness. Their approach is based on a genetic algorithm, and the notion of fairness that they consider is False-Positive Rate for avoiding disparate mistreatment, but it is possible to use most metrics available in the literature.
\item \cite{A comparative study of fairness-enhancing interventions in machine learning} provides empirical analysis of some of the existing fairness-enhancing methods for machine learning, showing that the results are influenced a lot by the fairness notion used and also by the dataset.
\end{enumerate}

\subsection{Accuracy $\times$ Privacy}

If we consider obfuscation methods for privacy (such as Differential-Privacy and Local Differential Privacy mechanisms), in general bigger privacy constraints imply in a smaller accuracy when training Machine Learning Models. There are also homomorphic encryption and secure multi-party computation approaches, which have computational complexity as a major challenge instead of the accuracy. In fact, some predictions can be made without any loss of accuracy at all, as shown in \cite{Privacy-preserving classification of customer data without loss of accuracy} for the naive Bayes classifier. Notice that in the Local Differential Privacy context, we know exactly how the noise is applied an thus might be able to reverse some the effect of the noise when training the model, and in general the effectiveness of this reversion determines how much the accuracy of the model will be affected.

\begin{enumerate}
\item \cite{Local differential privacy and its applications: A comprehensive survey} provides an overview of the use of Local Differential Privacy in general, but also further discusses machine learning in the local different privacy scenario, both for unsupervised and supervised learning.
\item \cite{Preserving User Privacy for Machine Learning: Local Differential Privacy or Federated Machine Learning?} compares Local Differential Privacy and Federated Learning approaches empirically, concluding that LDP approaches consume less computational resources on the client-side, benefits from a large user population and can be reused for other tasks, while the data transfered to the server for the Federated Learning approach is specific for one inference task.
\item \cite{LoPub : High-Dimensional Crowdsourced Data Publication With Local Differential Privacy} proposes a method for applying Local Differential Privacy to high-dimensional data, and evaluates it empirically, showing that in general stronger privacy also implies smaller accuracy.
\item \cite{About dependence between individuals: Dependence Makes You Vulnerable: Differential Privacy Under Dependent Tuples} explores differential privacy in the setting in which the individual data points are correlated in a way that can be explored by the adversary, and investigates how to provide better privacy guarantees in this scenarios. This might be important to consider in the machine learning scenario as well in the future, as the advesary might take advantage of such correlations in this type of situation as well.
\end{enumerate}

\subsection{Accuracy $\times$ Explainability}

If the Machine Learning model is inherentely interpretable by humans, then usually it is less accurate than more complex models (the most common example are the deep convolutional neural networks). Also, explainability methods might help the model developer in identifying problems and improving the model quality, which might include improving the accuracy \cite{A Survey on the Explainability of Supervised Machine Learning}. Also, for simpler models it might be possible to keep good accuracy and explainability levels.

\begin{enumerate}
\item \cite{It is Not Accuracy vs. Explainability We Need Both for Trustworthy AI Systems} discusses some arguments for and against this dilemma, and points out some options of where the focus should be when developing and implementing Machine Learning systems.
\item \cite{Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability} argues that in some situations it might be better to accept results without an explanation when these results can be empirically verified to lead to better results than well-explained results. Two important points raised by the paper are: \qm{The opacity, independence from an explicit domain model, and lack of causal insight associated with some powerful machine learning approaches are not radically different from routine aspects of medical decision-making} and \qm{In medicine, the ability to intervene effectively in the world by exploiting causal relationships often derives from experience and precedes clinicians' ability to understand why interventions work.}
\item \cite{Explainable artificial intelligence: an analytical review} discusses the known trade-offs between accuracy and explainability for some distinct scenarios and models, the areas that can benefit the most from explainable ML and a general review of explainability.
\item \cite{It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy} and \cite{Does Explainable Artificial Intelligence Improve Human Decision-Making?} provide an empirical study on whether explanations for AI predictions do or do not improve the accuracy of human decision-making, and the results indicate that this might not be the case.
\item \cite{Trading off accuracy and explainability in AI decision-making: findings from 2 citizens' juries} provides an empirical study to evaluate the opinion of the general public on whether it is worth trading explainability for accuracy in healthcare and non-healthcare scenarios. The conclusion is that in healthcare accuracy is favored, and in other scenarios explainability is valued equally to or more than accuracy.
\end{enumerate}

\subsection{Accuracy $\times$ Causality}

Accuracy is about how much a machine learning model makes correct guesses, Causality is about studying the causal relationships between variables. We can say that the problem of getting good accuracy lies on the first level of causation, as we are concerned about the data, although causality concepts could be used to transfer what was learned with the data of one population to another, for instance. Another possible relation is evaluating how accurate are causal discovery and causal inference algorithms. In general, these two concepts are unrelated.

\subsection{Accuracy $\times$ QIF}

Acuracy can be viewed as a form of utility, which represents how useful a system is, which might be affected by how vulnerable it is. It is trivial to develop a system without any vulnerability, for instance, a system that always outputs $0$ no matter what is the input. But it's not useful, so we can consider that QIF is related to accuracy when we consider the former as the study of information leakage from a system and the latter as a form of measuring utility.

\subsection{Fairness $\times$ Privacy}

There are some impossibility results in the literature that argue why it is impossible to achieve both pricacy and group fairness constraints under non-trivial accuracy\cite{On the Compatibility of Privacy and Fairness}. We also have positive results that show how stasifying privacy constraints can help mitigating group fairness\cite{A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results}\cite{On the impact of multi-dimensional local differential privacy on fairness}\cite{(Local) Differential Privacy has NO Disparate Impact on Fairness}, and we also have similar positive results results for individual fairness notions\cite{Awareness}.

\begin{enumerate}
\item \cite{A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results} aims to provide theoretical results that relate how training a machine learning model on a dataset in which local differential privacy mechanisms were applied can impact the fairness of this model, if the data-gatherer does not try to reverse the applied noise. The results are provided for a simplified machine learning model, from a theoretical persepctive.
\item \cite{On the impact of multi-dimensional local differential privacy on fairness} aims to evaluate experimentally how training a machine learning model on a multi-dimensional data set in which local differential privacy mechanisms were applied can impact the fairness of the model, again if the data-gatherer does not try to reverse the noise.
\item \cite{(Local) Differential Privacy has NO Disparate Impact on Fairness} executes an empirical evaluation of the impact of many Local Differential Privacy mechanisms on fairness when we do not try to reverse the applied noise, including a new privacy budget allocation scheme based on the domain size of sensitive attributes.
\item \cite{Awareness} introduces a notion of individual fairness that is a generalization of differential privacy, and explores the relationship between this notion of fairness, differential privacy and statistical disparity.
\item \cite{On the Compatibility of Privacy and Fairness} present theoretical results that show the impossibility of having a classifier that is not trivially accurate and at the same time satisfies $(\epsilon,0)$-differential privacy and equality of opportunity constraints. The paper also shows a PAC learner for an approximate fairness definition they provide.
\item \cite{Exploring fairness and privacy in machine learning} compiles results that relate the concetps of privacy and fairness, and also causal discovery.
\item \cite{Toward Learning Trustworthily from Data Combining Privacy, Fairness, and Explainability: An Application to Face Recognition} discusses the use of Homeomorphic Encryption in combination with fair representation learning \cite{Learning fair representations} to provide both privacy and fairness guarantees, respectively, when training machine learning models. Then the paper discusses how to provide local and global explanations for the model.
\end{enumerate}

\subsection{Fairness $\times$ Explainability}

Fairness evaluation is a goal usually mentioned when the utility of explainability studies is discussed in the literature\cite{"Just" accuracy? Procedural fairness demands explainability in AI-based medical resource allocations}. There are also some negative results in this respect, that indicate that in some situations, it is possible to provide convincingly fair explanations for decisions that were actually made in an unfair way\cite{How to explain and justify almost any decision: Potential pitfalls for accountability in ai decision-making}.

\begin{enumerate}
\item \cite{"Just" accuracy? Procedural fairness demands explainability in AI-based medical resource allocations} discusses how not providing explanations for decision can threaten accountability, bias avoidance and transparency when evaluating and mitigating unfairness in ML-based decisions.
\item \cite{How to explain and justify almost any decision: Potential pitfalls for accountability in ai decision-making} shows empirical results of training a baseline, a fair, an unfair and a random model on the COMPAS\cite{Compass} dataset and then finding possible explanations for the results provided by the model. The paper explores a space of explanations that are simple, verifiable and relevant according to definitions they provide for these terms, and show that it's possible to provide valid justifications for the decisions for the majority of the models. They also explore the situation in which explanations are provided for many data points and then analysed to verify for consistency, sufficiency and uniqueness (which they define formally in the paper), and achieve similar results: it can be possible to provide justifications for almost any decision that seem valid, even if the justification does not reflect how the model really decides.
\item \cite{Toward Learning Trustworthily from Data Combining Privacy, Fairness, and Explainability: An Application to Face Recognition} discusses the use of Homeomorphic Encryption in combination with fair representation learning \cite{Learning fair representations} to provide both privacy and fairness guarantees, respectively, when training machine learning models. Then the paper discusses how to provide local and global explanations for the model.
\end{enumerate}

\subsection{Fairness $\times$ Causality}

Fairness might be considered a causal concept: we want to know whether the outcome was or was not caused by an unfair process. The causal distinction might be important because, for instance, we might consider some causes fair and others unfair for the decision of not hiring someone. If the cause of the decision was the educational background and work experience of the person, it is usually considered fair, but if it is the skin color of the person it is usually considered unfair. There are many different definitions of causal notions of fairness, each of it's own specific meaning, as with causal notions in general.

\begin{enumerate}
\item \cite{Causality} mentions notions of sex discrimination in college admissions and discrimination in hiring. Pearl also argues why the causal interpretation should be used instead of purely statistical notions.
\item \cite{when causality meets fairness: a survey} and \cite{A review of causality-based fairness machine learning} survey many of the causal definitions of fairness present in the literature, \cite{when causality meets fairness: a survey} also classifies them according to Pearl's levels of causation and guidelines for which situations best suit each notion.
\item \cite{The Impossibility Theorem of Machine Fairness: a Causal Perspective} shows a causal interpretation of notions of fairness and approaches the conflict between different fairness notions from this perpective. One important note is that this paper does not use the correct notion of equalized odds, it reverses $Y$ and $\hat{Y}$: the correct notion is that the prediction is independent of the sensitive attriute given the ground-truth, but they consider the definition of the output being independent of the sensitive attribute given the prediction. The correct notion of equalized odds is what they call predictive parity. \emph{This paper was found in \url{arxiv.org}}.
\item \cite{Identifiability of Causal-based ML Fairness Notions} compiles major identifiability results from the causality literature and also discusses how they can be used in practice to obtain causal knowledge from data.
\end{enumerate}

\subsection{Fairness $\times$ QIF}

One possibility of measuring fairness with Quantitative Information Flow notions is to measure the \emph{reverse flow}: instead of looking at how much observing the output of a model helps in estimating input values (this would be a privacy concern), we measure how much observing the sensitive attribute helps in estimating output values. This idea is explored in \cite{On the duality of privacy and fairness} and \cite{On the relation of privacy and fairness through the lenses of quantitative information flow}.

\subsection{Privacy $\times$ Explainability}

One of the main questions of the relation between Privacy and Explainability is whether explainability is affected when privacy-preserving mechanisms are applied. Some results indicate that this is not always the case, but the conceptual differences are discussed times in more detail througout the literature. 

\begin{enumerate}
\item \cite{Privacy and Explainability: The Effects of Data Protection on Shapley Values} evaluates the effect of applying privacy protection mechanisms on Shapley values, which can be used for explanation purposes, both local and global. The conclusion is that applying privacy-protection mechanisms does not affect much explanations based on Shapley values.
\item \cite{Show us the data: Privacy, explainability, and why the law can't have both} discusses the legislation of privacy and explainability, arguing that we should not have both in the law, as they claim that demanding explanations can inevitably lead to demands on the visualization of the training data itself.
%\item \cite{AI Explainability, Interpretability, Fairness, and Privacy: An Integrative Review of Reviews} seems to provide a systematic review between these areas, {\color{red}but I do not have access to it.}
\item \cite{SoK: Taming the Triangle--On the Interplays between Fairness, Interpretability and Privacy in Machine Learning} points some conflicts and synergies that involve the two concepts: the conflicts include the conceptual difference between demanding more transparency in respect to the proccess and demanding secrecy in respect to the training data, which is part of the training proccess of the model, the possibility of using data obtained from explainability methods to infer sensitive inforamtion. The synnergies involve the possibility of using inforamtion obtained from explainability methods to improve the privacy of the model, and the fact that some results in the literature claim to have achieved good explainability for some models while keeping acceptable privacy constraints.
\item \cite{Toward Learning Trustworthily from Data Combining Privacy, Fairness, and Explainability: An Application to Face Recognition} discusses the use of Homeomorphic Encryption in combination with fair representation learning \cite{Learning fair representations} to provide both privacy and fairness guarantees, respectively, when training machine learning models. Then the paper discusses how to provide local and global explanations for the model.
\end{enumerate}


\subsection{Privacy $\times$ Causality}

In regard to the relationship between privacy and causality, one approach in the literature is to try to provide privacy for sensitive data by the addition of noise, and at the same time allow causal discovery and causal inference based on the noisy data. Another relation noted in the literature is that models based on causal relationships between variables are more resilient to privacy attacks, such that privacy-protection mechanisms are more effective for this type of model. Another possibility is modeling differential privacy as a causal, instead of purely statistical, property. 

\begin{enumerate}
\item \cite{Causal Discovery Under Local Privacy} discusses local differential privacy in the context of causal discovery: how local differential privacy mechanisms affect the task of causal discovery. The conclusion is that the geometric mechanism has a smaller effect than the methods based on generalizations of the Randomized Response mechanism, as they tend to degrade less the original correlations present in the data. It seems that this paper also doesn't try to recover the original distribution from the noisy one before trying causal discovery...
\item \cite{Alleviating Privacy Attacks via Causal Learning} shows that models that use only the parents of the target variable, which they call causal models, provide stronger $\epsilon$-differential privacy guarantees, are provably more robust to Membership Inference Attacks, generalize better to other distributions based on the same causal structure.
\item \cite{SoK: Differential Privacy as a Causal Property} discusses how differential privacy can be interpreted as a causal notion and what are the benefits of this interpretation. They analyse some possible causal and associational interpretations of the intuitive idea of Differential Privacy. The causal interpretations consist of interventions on all data points or one data point, and they distinguish between an individual's data point and the real value of the individual. They show the equivalence between many of the definitions they present and the unidirectional implications of two of them. In summary:
\item \begin{enumerate}
    \item Differential privacy constraints on an algorithm that takes many data points as input can be interpreted as requiring that the output value of this algorithm does not change if we change only one data point. This will be considered the definition of Differential Privacy.
    \item This is equivalent to requiring that for any input distributions the probability of any outcome of the algorithm is the similar if one data point was observed to be different and the rest equal.
    \item This is equivalent to requiring that for any distribution with independent data points such that any individual data point value is possible, the probability of any outcome of the algorithm is the same if one data point was observed to be different.
    \item Requiring the same as above without the assumption of independent data points is a stronger requirement than differential privacy (it implies DP, but not the opposite).
    \item This is also equivalent to causal notions that require that intervening to change one value and \emph{intervening} to keep the others constant do not change much the probability of any outcome. This holds regardless of whether we consider that for an algorithm to satisfy differential privacy this must hold for all distributions or for only one (as the interventions are done on \emph{all} variables).
    \item This is also equivalent to the causal notion that interevening on one variable should not change much the probability of each output, if we consider this must hold for all probability distributions.
    \item If we consider the previous definition but without requiring to hold for all distributions, then for technical reasons of specific zero probability distributions, this is weaker than differential privacy (DP implies this definition, but not the opposite).
    \end{enumerate}
\item \cite{Differential Privacy Preserving Causal Graph Discovery} discusses {\color{red} AQUI CONFERIR PARECE QUE EH MAIS SOBRE COMO GARANTIR QUE A DIVULGACAO DOS RESULTADOS DA CAUSAL GRAPH DISCOVEY NAO VAI VIOLAR A PRIVACIDADE DOS USUARIOS}
\item \cite{Private Causal Inference} 
\end{enumerate}

\subsection{Privacy $\times$ QIF}

{\color{red} QIF was desined to work with privacy, quantifying how much the sensitive information is leaking is in a way quantifying privacy.}

\subsection{Explainability $\times$ Causality}

{\color{red} Causality is inheretely easier to explain.}

\subsection{Explainability $\times$ QIF}

{\color{red} No idea! Maybe a way to see which variables leak more information?}

\subsection{Causality $\times$ QIF}

{\color{red} I don't think that the paper I saw was published, but maybe we can mention that.}

