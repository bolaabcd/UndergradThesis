\subsection{Accuracy $\times$ Fairness}

There are results that indicate an inherent trade-off between fairness and accuracy in machine learning:

\begin{enumerate}
\item \cite{Carlos} shows that there are trade-offs between Equal Oportunity Difference and accuracy such that, depending on the data distribution, it might impossible to achieve both perfect Equal Oportunity Difference and non-trivial accuracy. It also shows some other theoretical results that associate EOD and accuracy, such as sufficient conditions for the existance of non-trivially accurate predictors that lead to zero Equal Opportunity Difference and non-trivial accuracy and algebraic and geometric properties of the feasible values of Equal Opportunity Difference and accuracy.
\item \cite{Reductions} provides methods for computing the best possible accuracy given some level of fairness, for a general notion of fairness that encompasses many common metrics. They devise an algorithm for solving a constrained linear optimization problem that minimizes the error subject to fairness constraints, and provide experimental results for some datasets, including the COMPAS \cite{Compass} dataset. One interesting results is that for some datasets (such as the Compas dataset), it is possible to reduce Equal Opportunity Difference without changing much the accuracy, but for some datasets (such as the Dutch Census Dataset\cite{merono2017cedar} with gender as the protected attribute and the goal is if someone has a prestigious occupation). {\color{orange} precisava citar o dataset assim?}
\item \cite{konstantinov2022impossibility} shows that it is always possible for an adversary to corrupt the data available for a learning algorithm in a way that the algorithm is unfair, and sometimes it is possible to do so without changing the accuracy.
\item \cite{valdivia2021fair} Provides a method of finding the full Paretto front of accuracy versus fairness. Their approach is based on a genetic algorithm, and the notion of fairness that they consider is False-Positive Rate for avoiding disparate mistreatment, but it is possible to use most metrics available in the literature.
\item \cite{friedler2019comparative} provides empirical analysis of some of the existing fairness-enhancing methods for machine learning, showing that the results are influenced a lot by the fairness notion used and also by the dataset.
\end{enumerate}

\subsection{Accuracy $\times$ Privacy}

If we consider obfuscation methods for privacy (such as Differential-Privacy and Local Differential Privacy mechanisms), in general bigger privacy constraints imply in a smaller accuracy when training Machine Learning Models. There are also homomorphic encryption and secure multi-party computation approaches, which have computational complexity as a major challenge instead of the accuracy. In fact, some predictions can be made without any loss of accuracy at all, as shown in \cite{yang2005privacy} for the naive Bayes classifier. Notice that in the Local Differential Privacy context, we know exactly how the noise is applied an thus might be able to reverse some the effect of the noise when training the model, and in general the effectiveness of this reversion determines how much the accuracy of the model will be affected.

\begin{enumerate}
\item \cite{yang2023local} provides an overview of the use of Local Differential Privacy in general, but also further discusses machine learning in the local different privacy scenario, both for unsupervised and supervised learning.{\color{orange} Em alguns desses primeiros papers eu só menciono que o paper fala sobre um assunto, mas não falo sobre o assunto eu mesmo. Era importante eu aprofundar um pouco mais nesses casos?}

\item \cite{zheng2020preserving} compares Local Differential Privacy and Federated Learning approaches empirically, concluding that LDP approaches consume less computational resources on the client-side, benefits from a large user population and can be reused for other tasks, while the data transfered to the server for the Federated Learning approach is specific for one inference task.
\item \cite{ren2018textsf} proposes a method for applying Local Differential Privacy to high-dimensional data, and evaluates it empirically, showing that in general stronger privacy also implies smaller accuracy.
\item \cite{liu2016dependence} explores differential privacy in the setting in which the individual data points are correlated in a way that can be explored by the adversary, and investigates how to provide better privacy guarantees in this scenarios. This might be important to consider in the machine learning scenario as well in the future, as the advesary might take advantage of such correlations in this type of situation as well.
\item \cite{yang2005privacy} discusses how to use homomorphic encryption to preserve privacy without loss of accuracy. The classification algorithm used as an example is the naive bayes classifier,
\end{enumerate}

\subsection{Accuracy $\times$ Explainability}

If the Machine Learning model is inherentely interpretable by humans, then usually it is less accurate than more complex models (the most common example are the deep convolutional neural networks). Also, explainability methods might help the model developer in identifying problems and improving the model quality, which might include improving the accuracy \cite{burkart2021survey}. Also, for simpler models it might be possible to keep good accuracy and explainability levels.

\begin{enumerate}
\item \cite{petkovic2023not} discusses some arguments for and against this dilemma, and points out some options of where the focus should be when developing and implementing Machine Learning systems.
\item \cite{london2019artificial} argues that in some situations it might be better to accept results without an explanation when these results can be empirically verified to lead to better results than well-explained results. Two important points raised by the paper are: \qm{The opacity, independence from an explicit domain model, and lack of causal insight associated with some powerful machine learning approaches are not radically different from routine aspects of medical decision-making} and \qm{In medicine, the ability to intervene effectively in the world by exploiting causal relationships often derives from experience and precedes clinicians' ability to understand why interventions work.}
\item \cite{angelov2021explainable} discusses the known trade-offs between accuracy and explainability for some distinct scenarios and models, the areas that can benefit the most from explainable ML and a general review of explainability.
\item \cite{bell2022s} and \cite{alufaisan2021does} provide an empirical study on whether explanations for AI predictions do or do not improve the accuracy of human decision-making, and the results indicate that this might not be the case.
\item \cite{van2021trading} provides an empirical study to evaluate the opinion of the general public on whether it is worth trading explainability for accuracy in healthcare and non-healthcare scenarios. The conclusion is that in healthcare accuracy is favored, and in other scenarios explainability is valued equally to or more than accuracy.
\end{enumerate}

\subsection{Accuracy $\times$ Causality}

Accuracy is about how much a machine learning model makes correct guesses, Causality is about studying the causal relationships between variables. We can say that the problem of getting good accuracy lies on the first level of causation, as we are concerned about the data, although we could, for instance, use causality concepts to transfer what was learned with the data of one population to another. In some situations, algorithms based on causal notions can be even more accurate than purely associative algorithm.

\begin{enumerate}
    \item \cite{richens2020improving} presents a counterfactual-based algorithm for medical diagnosis when there are many possible causes for a patient's symptoms. Their algorithm results in better accuracy than some classical Machine Learning algorithms not based on causal notions.
    \item \cite{scholkopf2022causality} discusses many relations between causal learning and machine learning, including references to works that discuss the idea of transfering results to different populations without losing too much accuracy.
    \item \cite{zeng2021improving} present a method of selecting features with causal analysis before training a machine learning model based on causal concepts, and show that this approach can improve accuracy of network intrusion detection when compared to classical machine learning methods.
\end{enumerate}

\subsection{Accuracy $\times$ QIF}

Acuracy can be viewed as a form of utility, which represents how useful a system is, which might be affected by how vulnerable it is. It is trivial to develop a system without any vulnerability, for instance, a system that always outputs $0$ no matter what is the input. But it's not useful, so we can consider that QIF is related to accuracy when we consider the former as the study of information leakage from a system and the latter as a form of measuring utility.

\subsection{Fairness $\times$ Privacy}

There are some impossibility results in the literature that argue why it is impossible to achieve both pricacy and group fairness constraints under non-trivial accuracy\cite{Rachel}. We also have positive results that show how stasifying privacy constraints can help mitigating group unfairness\cite{makhlouf2024systematicformalstudyimpact}\cite{makhlouf2024impact}\cite{arcolezi2023local}, and we also have similar positive results results for individual fairness notions\cite{Awareness}.

\begin{enumerate}
\item \cite{makhlouf2024systematicformalstudyimpact} aims to provide theoretical results that relate how training a machine learning model on a dataset in which local differential privacy mechanisms were applied can impact the fairness of this model, if the data-gatherer does not try to reverse the applied noise. The results are provided for a simplified machine learning model, from a theoretical persepctive.
\item \cite{makhlouf2024impact} aims to evaluate experimentally how training a machine learning model on a multi-dimensional data set in which local differential privacy mechanisms were applied can impact the fairness of the model, again if the data-gatherer does not try to reverse the noise.
\item \cite{arcolezi2023local} executes an empirical evaluation of the impact of many Local Differential Privacy mechanisms on fairness when we do not try to reverse the applied noise, including a new privacy budget allocation scheme based on the domain size of sensitive attributes.
\item \cite{Awareness} introduces a notion of individual fairness that is a generalization of differential privacy, and explores the relationship between this notion of fairness, differential privacy and statistical disparity.
\item \cite{Rachel} present theoretical results that show the impossibility of having a classifier that is not trivially accurate and at the same time satisfies $(\epsilon,0)$-differential privacy and equality of opportunity constraints. The paper also shows a PAC learner for an approximate fairness definition they provide.
\item \cite{henao2023exploring} compiles results that relate the concetps of privacy and fairness, and also causal discovery.
\item \cite{franco2021toward} discusses the use of Homomorphic Encryption in combination with fair representation learning \cite{zemel2013learning} to provide both privacy and fairness guarantees, respectively, when training machine learning models. Then the paper discusses how to provide local and global explanations for the model.
\end{enumerate}

\subsection{Fairness $\times$ Explainability}

Fairness evaluation is a goal usually mentioned when the utility of explainability studies is discussed in the literature\cite{rueda2022just}. There are also some negative results in this respect, that indicate that in some situations, it is possible to provide convincingly fair explanations for decisions that were actually made in an unfair way\cite{ExplainAll}.

\begin{enumerate}
\item \cite{rueda2022just} discusses how not providing explanations for decision can threaten accountability, bias avoidance and transparency when evaluating and mitigating unfairness in ML-based decisions.
\item \cite{ExplainAll} shows empirical results of training a baseline, a fair, an unfair and a random model on the COMPAS\cite{Compass} dataset and then finding possible explanations for the results provided by the model. The paper explores a space of explanations that are simple, verifiable and relevant according to definitions they provide for these terms, and show that it's possible to provide valid justifications for the decisions for the majority of the models. They also explore the situation in which explanations are provided for many data points and then analysed to verify for consistency, sufficiency and uniqueness (which they define formally in the paper), and achieve similar results: it can be possible to provide justifications for almost any decision that seem valid, even if the justification does not reflect how the model really decides.
\item \cite{franco2021toward} discusses the use of Homomorphic Encryption in combination with fair representation learning \cite{zemel2013learning} to provide both privacy and fairness guarantees, respectively, when training machine learning models. Then the paper discusses how to provide local and global explanations for the model.
\end{enumerate}

\subsection{Fairness $\times$ Causality}

Fairness might be considered a causal concept: we want to know whether the outcome was or was not caused by an unfair process. The causal distinction might be important because, for instance, we might consider some causes fair and others unfair for the decision of not hiring someone. If the cause of the decision was the educational background and work experience of the person, it is usually considered fair, but if it is the skin color of the person it is usually considered unfair. There are many different definitions of causal notions of fairness, each of it's own specific meaning, as with causal notions in general.

\begin{enumerate}
\item \cite{Causality} mentions notions of sex discrimination in college admissions and discrimination in hiring. Pearl also argues why the causal interpretation should be used instead of purely statistical notions.
\item \cite{makhlouf2024causality} and \cite{su2022review} survey many of the causal definitions of fairness present in the literature, \cite{makhlouf2024causality} also classifies them according to Pearl's levels of causation and guidelines for which situations best suit each notion.
\item \cite{saravanakumar2020impossibility} shows a causal interpretation of notions of fairness and approaches the conflict between different fairness notions from this perpective. One important note is that this paper does not use the correct notion of equalized odds, it reverses $Y$ and $\hat{Y}$: the correct notion is that the prediction is independent of the sensitive attriute given the ground-truth, but they consider the definition of the output being independent of the sensitive attribute given the prediction. The correct notion of equalized odds is what they call predictive parity. \emph{This paper was found in \url{arxiv.org}}.
\item \cite{makhlouf2022identifiability} compiles major identifiability results from the causality literature and also discusses how they can be used in practice to obtain causal knowledge from data.
\end{enumerate}

\subsection{Fairness $\times$ QIF}

One possibility of measuring fairness with Quantitative Information Flow notions is to measure the \emph{reverse flow}: instead of looking at how much observing the output of a model helps in estimating input values (this would be a privacy concern), we measure how much observing the sensitive attribute helps in estimating output values. This idea is explored in \cite{Bruno} and \cite{nogueira2023relation}.

\subsection{Privacy $\times$ Explainability}

One of the main questions of the relation between Privacy and Explainability is whether explainability is affected when privacy-preserving mechanisms are applied. Some results indicate that this is not always the case, but the conceptual differences are discussed times in more detail througout the literature. 

\begin{enumerate}
\item \cite{bozorgpanah2022privacy} evaluates the effect of applying privacy protection mechanisms on Shapley values, which can be used for explanation purposes, both local and global. The conclusion is that applying privacy-protection mechanisms does not affect much explanations based on Shapley values.
\item \cite{grant2020show} discusses the legislation of privacy and explainability, arguing that we should not have both in the law, as they claim that demanding explanations can inevitably lead to demands on the visualization of the training data itself.
%\item \cite{AI Explainability, Interpretability, Fairness, and Privacy: An Integrative Review of Reviews} seems to provide a systematic review between these areas, {\color{red}but I do not have access to it.}
\item \cite{Sok} points some conflicts and synergies that involve the two concepts: the conflicts include the conceptual difference between demanding more transparency in respect to the proccess and demanding secrecy in respect to the training data, which is part of the training proccess of the model, the possibility of using data obtained from explainability methods to infer sensitive inforamtion. The synnergies involve the possibility of using inforamtion obtained from explainability methods to improve the privacy of the model, and the fact that some results in the literature claim to have achieved good explainability for some models while keeping acceptable privacy constraints.
\item \cite{franco2021toward} discusses the use of Homomorphic Encryption in combination with fair representation learning \cite{zemel2013learning} to provide both privacy and fairness guarantees, respectively, when training machine learning models. Then the paper discusses how to provide local and global explanations for the model.
\end{enumerate}


\subsection{Privacy $\times$ Causality}

In regard to the relationship between privacy and causality, one approach in the literature is to try to provide privacy for sensitive data by the addition of noise, and at the same time allow causal discovery and causal inference based on the noisy data. Another relation noted in the literature is that models based on causal relationships between variables are more resilient to privacy attacks, such that privacy-protection mechanisms are more effective for this type of model. Another possibility is modeling differential privacy as a causal, instead of purely statistical, property. 

\begin{enumerate}
\item \cite{binkyte2024causal} discusses local differential privacy in the context of causal discovery: how local differential privacy mechanisms affect the task of causal discovery. The conclusion is that the geometric mechanism has a smaller effect than the methods based on generalizations of the Randomized Response mechanism, as they tend to degrade less the original correlations present in the data. It seems that this paper also doesn't try to recover the original distribution from the noisy one before trying causal discovery...
\item \cite{tople2020alleviating} shows that models that use only the parents of the target variable, which they call causal models, provide stronger $\epsilon$-differential privacy guarantees, are provably more robust to Membership Inference Attacks, generalize better to other distributions based on the same causal structure.
\item \cite{tschantz2020sok} discusses how differential privacy can be interpreted as a causal notion and what are the benefits of this interpretation. They analyse some possible causal and associational interpretations of the intuitive idea of Differential Privacy. The causal interpretations consist of interventions on all data points or one data point, and they distinguish between an individual's data point and the real value of the individual. They show the equivalence between many of the definitions they present and the unidirectional implications of two of them. In summary: \begin{enumerate}
    \item Differential privacy constraints on an algorithm that takes many data points as input can be interpreted as requiring that the output value of this algorithm does not change if we change only one data point. This will be considered the definition of Differential Privacy.
    \item This is equivalent to requiring that for any input distributions the probability of any outcome of the algorithm is the similar if one data point was observed to be different and the rest equal.
    \item This is equivalent to requiring that for any distribution with independent data points such that any individual data point value is possible, the probability of any outcome of the algorithm is the same if one data point was observed to be different.
    \item Requiring the same as above without the assumption of independent data points is a stronger requirement than differential privacy (it implies DP, but not the opposite).
    \item This is also equivalent to causal notions that require that intervening to change one value and \emph{intervening} to keep the others constant do not change much the probability of any outcome. This holds regardless of whether we consider that for an algorithm to satisfy differential privacy this must hold for all distributions or for only one (as the interventions are done on \emph{all} variables).
    \item This is also equivalent to the causal notion that interevening on one variable should not change much the probability of each output, if we consider this must hold for all probability distributions.
    \item If we consider the previous definition but without requiring to hold for all distributions, then for technical reasons of specific zero probability distributions, this is weaker than differential privacy (DP implies this definition, but not the opposite).
    \end{enumerate}
\item \cite{xu2017differential} discusses new methods to pursue causal discovery and at the same time mantain differential privacy guarantees. The core of the idea considered is to obtain some independence information from the original data in order to reduce the privacy budget necessary for differential privacy. The focus of this paper is classical differential privacy instead of local differential privacy.
\item \cite{kusner2016private} discusses causal inference instead of causal discovery under differential privacy guarantees, for the causal discovery framework of Additive Noise Model.
\end{enumerate}

\subsection{Privacy $\times$ QIF}

As QIF aims to quantify the flow of information, we can naturally consider applying it to the privacy scenario, by measuring how much information leaks about an arbitrary individual. One general idea is that differential privacy is a worst-case notion and $g$-vulnerability is an average-case notion, and there are results\cite{QIF} showing that differential privacy implies bounds on leakage under arbitrary gain functions and prior distributions, but not the opposite.

\begin{enumerate}
\item \cite{alvim2015information} discusses the relations between differential privacy and $g$-vulnerability, including bounds on $g$-leakage as a function of the $\epsilon$ parameter of differential privacy, and the fact that there is no bound on differential privacy as a function of the $g$-vulnerability.

\item \cite{fernandes2022explaining} discusses how the $\epsilon$ parater of Differential Privacy is related to max-case $g$-vulnerability: $e^\epsilon$ is exactly the multiplicative max-case channel capacity under a fixed prior. This work also discusses many other theoretical results relating $g$-vulnerability notions with differential privacy. \emph{Found in \url{arxiv.org}}.
\end{enumerate}

\subsection{Explainability $\times$ Causality}

Causal models are considered inheretly interpretable, as the causal relationships between variables are explicitly represented in the model. Many machine learning explanation methods are based only on the data and thus based on the correlations between variables, so they usually lack a causal basis, which is a recurrent critic to such methods. So, another possible relation between the two concepts is to apply causal tools and measures to improve the quality of the machine learning explanations.

\begin{enumerate}
\item \cite{vowels2022trying} discusses how machine learning models in combination with explainability methods may not capture all the relevant correlations between the variables. As mentioned in the paper these explanations are \emph{local to the model}, and the paper shows that they important correlations in the data might not be captured by ML models trained with the data. \emph{This paper was found in }\url{arxiv.org}.
\item \cite{ma2023xinsight} discusses a method of providing causal-based explanations for some types of user queries. The idea is to estimate a causal graph from the data and answer queries of why some effect is observed by estimating the causal effect of each variable. The causal notion used is Database Causality, an extension of Pearl's Actual Causality.
\item \cite{carloni2023role} provides a broad literature review of the relations between causality and explainability. They identify three main connections between the two concepts present in the literature: \begin{enumerate}
    \item Some works present critics to Explainable AI through a causal perspective, which involve mainly the fact that many explainability approaches lack a causal interpretation, others discuss problems with non-causal explanations in general, and others discuss the adequate form of presenting explanations.
    \item Some works support explainability as a basis for further causal investigation, which might be done empirically.
    \item Finally, some works adhere to the idea of using causal tools to support machine learning model explanations, for instance the use of the $do$ operator, the estimation of the probability of necessity and the probability of sufficiency etc.. Causal counterfactual explanations are also considered, and some works consider simply providing a coherent causal model of a system to be a form of explaining the system itself.
    \end{enumerate}
\end{enumerate}

\subsection{Explainability $\times$ QIF}

It might be possible to create an explanation method based on how much information flows through each input to the outputs of a machine learning model. The relation between these two concepts remain largely unexplored. There is, though, an approach by \cite[Part~III]{calin2020deep} that uses an alternative definition of information based on sigma algebras and brings concepts from measure theory to provide some theoretical results involving deep learning.

\subsection{Causality $\times$ QIF}

Some unpublished preliminary work indicate that there might be a relation between causal discovery and Quantitative Information Flow metrics. But the relation between these two concepts still remains largely unexplored. {\color{orange} Faz sentido mencionar isso?}
