\subsection{Accuracy $\times$ Fairness}

There are results that indicate an inherent trade-off between fairness and accuracy in machine learning:

\begin{enumerate}
\item \cite{Carlos} shows that there are trade-offs between Equal Oportunity Difference and accuracy such that, depending on the data distribution, it might impossible to achieve both perfect Equal Oportunity Difference and non-trivial accuracy. It also shows some other theoretical results that associate EOD and accuracy, such as sufficient conditions for the existance of non-trivially accurate predictors that lead to zero Equal Opportunity Difference and non-trivial accuracy and algebraic and geometric properties of the feasible values of Equal Opportunity Difference and accuracy.
\item \cite{Reductions} provides methods for computing the best possible accuracy given some level of fairness, for a general notion of fairness that encompasses many common metrics. They devise an algorithm for solving a constrained linear optimization problem that minimizes the error subject to fairness constraints, and provide experimental results for some datasets, including the COMPAS \cite{Compass} dataset. One interesting results is that for some datasets (such as the Compas dataset), it is possible to reduce Equal Opportunity Difference without changing much the accuracy, but for some datasets (such as the Dutch Census Dataset\cite{Dutch} with gender as the protected attribute and the goal is if someone has a prestigious occupation) {\color{red} Dutch Census deve ser esse: \url{https://www.researchgate.net/profile/Eric-Schulte-Nordholt/publication/286099390_The_Dutch_Census_2011/links/58a3228b458515d15fd942d2/The-Dutch-Census-2011.pdf}}
\item \cite{How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness} Provides a method of finding the full Paretto front of accuracy versus fairness. Their approach is based on a genetic algorithm, and the notion of fairness that they consider is False-Positive Rate for avoiding disparate mistreatment, but it is possible to use most metrics available in the literature.
\item \cite{A comparative study of fairness-enhancing interventions in machine learning} provides empirical analysis of some of the existing fairness-enhancing methods for machine learning, showing that the results are influenced a lot by the fairness notion used and also by the dataset.
\end{enumerate}

\subsection{Accuracy $\times$ Privacy}

If we consider obfuscation methods for privacy (such as Differential-Privacy and Local Differential Privacy mechanisms), in general bigger privacy constraints imply in a smaller accuracy when training Machine Learning Models. There are also homomorphic encryption and secure multi-party computation approaches, which have computational complexity as a major challenge instead of the accuracy. In fact, some predictions can be made without any loss of accuracy at all, as shown in \cite{Privacy-preserving classification of customer data without loss of accuracy} for the naive Bayes classifier. Notice that in the Local Differential Privacy context, we know exactly how the noise is applied an thus might be able to reverse some the effect of the noise when training the model, and in general the effectiveness of this reversion determines how much the accuracy of the model will be affected.

\begin{enumerate}
\item \cite{Local differential privacy and its applications: A comprehensive survey} provides an overview of the use of Local Differential Privacy in general, but also further discusses machine learning in the local different privacy scenario, both for unsupervised and supervised learning.
\item \cite{Preserving User Privacy for Machine Learning: Local Differential Privacy or Federated Machine Learning?} compares Local Differential Privacy and Federated Learning approaches empirically, concluding that LDP approaches consume less computational resources on the client-side, benefits from a large user population and can be reused for other tasks, while the data transfered to the server for the Federated Learning approach is specific for one inference task.
\item \cite{LoPub : High-Dimensional Crowdsourced Data Publication With Local Differential Privacy} proposes a method for applying Local Differential Privacy to high-dimensional data, and evaluates it empirically, showing that in general stronger privacy also implies smaller accuracy.
\item \cite{About dependence between individuals: Dependence Makes You Vulnerable: Differential Privacy Under Dependent Tuples} explores differential privacy in the setting in which the individual data points are correlated in a way that can be explored by the adversary, and investigates how to provide better privacy guarantees in this scenarios. This might be important to consider in the machine learning scenario as well in the future, as the advesary might take advantage of such correlations in this type of situation as well.
\end{enumerate}

\subsection{Accuracy $\times$ Explainability}

{\color{red} Probably can find it in the explainability literature}

\subsection{Accuracy $\times$ Causality}

{\color{red} Maybe we can say that accuracy lies in the first ladder of causation, and usually we want to answer questions on other levels?}

\subsection{Accuracy $\times$ QIF}

{\color{red} Accuracy can be seen as a form of utility, maybe this is usefull for statistical disclosure control.}

\subsection{Fairness $\times$ Privacy}

{\color{red} Rachel's paper, fairness through awareness maybe, and others. Awareness is about being fair by a generalization of differential privacy if I recall correctly, at least in individual fairness, the relations with group fairness and other stuff.}

\begin{enumerate}
\item (Local) Differential Privacy has NO Disparate Impact on Fairness
\item On the impact of multi-dimensional local differential privacy on fairness
\item A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results
\item 
\end{enumerate}

\subsection{Fairness $\times$ Explainability}

{\color{red} Some people try to judge fairness based on the explanation, we can mention the How to Justify Almost Anything aqui, que pra auditar pode não ser uma boa ideia, só isso mesmo talvez.}

\subsection{Fairness $\times$ Causality}

{\color{red} Citar paper Karima sobre noções causais de fairness.}

\subsection{Fairness $\times$ QIF}

{\color{red} Citar trabalho do Bruno sobre QIF ao contrário como noção de fairness talvez}

\subsection{Privacy $\times$ Explainability}

{\color{red} No idea!}

\subsection{Privacy $\times$ Causality}

{\color{red} No idea! Maybe be private to causal discovery? Maybe be private but allow causal discovery? I think that there is a paper by Sylvia about this...}

\subsection{Privacy $\times$ QIF}

{\color{red} QIF was desined to work with privacy, quantifying how much the sensitive information is leaking is in a way quantifying privacy.}

\subsection{Explainability $\times$ Causality}

{\color{red} Causality is inheretely easier to explain.}

\subsection{Explainability $\times$ QIF}

{\color{red} No idea! Maybe a way to see which variables leak more information?}

\subsection{Causality $\times$ QIF}

{\color{red} I don't think that the paper I saw was published, but maybe we can mention that.}

