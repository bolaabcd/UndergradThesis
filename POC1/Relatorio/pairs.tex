\subsection{Accuracy $\times$ Fairness}

There are results that indicate an inherent trade-off between fairness and accuracy in machine learning:

\begin{enumerate}
\item \cite{Carlos} shows that there are trade-offs between Equal Oportunity Difference and accuracy such that, depending on the data distribution, it might impossible to achieve both perfect Equal Oportunity Difference and non-trivial accuracy. It also shows some other theoretical results that associate EOD and accuracy, such as sufficient conditions for the existance of non-trivially accurate predictors that lead to zero Equal Opportunity Difference and non-trivial accuracy and algebraic and geometric properties of the feasible values of Equal Opportunity Difference and accuracy.
\item \cite{Reductions} provides methods for computing the best possible accuracy given some level of fairness, for a general notion of fairness that encompasses many common metrics. They devise an algorithm for solving a constrained linear optimization problem that minimizes the error subject to fairness constraints, and provide experimental results for some datasets, including the COMPAS \cite{Compass} dataset. One interesting results is that for some datasets (such as the Compas dataset), it is possible to reduce Equal Opportunity Difference without changing much the accuracy, but for some datasets (such as the Dutch Census Dataset\cite{Dutch} with gender as the protected attribute and the goal is if someone has a prestigious occupation) {\color{red} Dutch Census deve ser esse: \url{https://www.researchgate.net/profile/Eric-Schulte-Nordholt/publication/286099390_The_Dutch_Census_2011/links/58a3228b458515d15fd942d2/The-Dutch-Census-2011.pdf}}
\item \cite{On the Impossibility of Fairness-Aware Learning from Corrupted Data} shows that it is always possible for an adversary to corrupt the data available for a learning algorithm in a way that the algorithm is unfair, and sometimes it is possible to do so without changing the accuracy.
\item \cite{How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness} Provides a method of finding the full Paretto front of accuracy versus fairness. Their approach is based on a genetic algorithm, and the notion of fairness that they consider is False-Positive Rate for avoiding disparate mistreatment, but it is possible to use most metrics available in the literature.
\item \cite{A comparative study of fairness-enhancing interventions in machine learning} provides empirical analysis of some of the existing fairness-enhancing methods for machine learning, showing that the results are influenced a lot by the fairness notion used and also by the dataset.
\end{enumerate}

\subsection{Accuracy $\times$ Privacy}

If we consider obfuscation methods for privacy (such as Differential-Privacy and Local Differential Privacy mechanisms), in general bigger privacy constraints imply in a smaller accuracy when training Machine Learning Models. There are also homomorphic encryption and secure multi-party computation approaches, which have computational complexity as a major challenge instead of the accuracy. In fact, some predictions can be made without any loss of accuracy at all, as shown in \cite{Privacy-preserving classification of customer data without loss of accuracy} for the naive Bayes classifier. Notice that in the Local Differential Privacy context, we know exactly how the noise is applied an thus might be able to reverse some the effect of the noise when training the model, and in general the effectiveness of this reversion determines how much the accuracy of the model will be affected.

\begin{enumerate}
\item \cite{Local differential privacy and its applications: A comprehensive survey} provides an overview of the use of Local Differential Privacy in general, but also further discusses machine learning in the local different privacy scenario, both for unsupervised and supervised learning.
\item \cite{Preserving User Privacy for Machine Learning: Local Differential Privacy or Federated Machine Learning?} compares Local Differential Privacy and Federated Learning approaches empirically, concluding that LDP approaches consume less computational resources on the client-side, benefits from a large user population and can be reused for other tasks, while the data transfered to the server for the Federated Learning approach is specific for one inference task.
\item \cite{LoPub : High-Dimensional Crowdsourced Data Publication With Local Differential Privacy} proposes a method for applying Local Differential Privacy to high-dimensional data, and evaluates it empirically, showing that in general stronger privacy also implies smaller accuracy.
\item \cite{About dependence between individuals: Dependence Makes You Vulnerable: Differential Privacy Under Dependent Tuples} explores differential privacy in the setting in which the individual data points are correlated in a way that can be explored by the adversary, and investigates how to provide better privacy guarantees in this scenarios. This might be important to consider in the machine learning scenario as well in the future, as the advesary might take advantage of such correlations in this type of situation as well.
\end{enumerate}

\subsection{Accuracy $\times$ Explainability}

If the Machine Learning model is inherentely interpretable by humans, then usually it is less accurate than more complex models (the most common example are the deep convolutional neural networks). Also, explainability methods might help the model developer in identifying problems and improving the model quality, which might include improving the accuracy \cite{A Survey on the Explainability of Supervised Machine Learning}. Also, for simpler models it might be possible to keep good accuracy and explainability levels.

\begin{enumerate}
\item \cite{It is Not Accuracy vs. Explainability We Need Both for Trustworthy AI Systems} discusses some arguments for and against this dilemma, and points out some options of where the focus should be when developing and implementing Machine Learning systems.
\item \cite{Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability} argues that in some situations it might be better to accept results without an explanation when these results can be empirically verified to lead to better results than well-explained results. Two important points raised by the paper are: \qm{The opacity, independence from an explicit domain model, and lack of causal insight associated with some powerful machine learning approaches are not radically different from routine aspects of medical decision-making} and \qm{In medicine, the ability to intervene effectively in the world by exploiting causal relationships often derives from experience and precedes clinicians' ability to understand why interventions work.}
\item \cite{Explainable artificial intelligence: an analytical review} discusses the known trade-offs between accuracy and explainability for some distinct scenarios and models, the areas that can benefit the most from explainable ML and a general review of explainability.
\item \cite{It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy} and \cite{Does Explainable Artificial Intelligence Improve Human Decision-Making?} provide an empirical study on whether explanations for AI predictions do or do not improve the accuracy of human decision-making, and the results indicate that this might not be the case.
\item \cite{Trading off accuracy and explainability in AI decision-making: findings from 2 citizens' juries} provides an empirical study to evaluate the opinion of the general public on whether it is worth trading explainability for accuracy in healthcare and non-healthcare scenarios. The conclusion is that in healthcare accuracy is favored, and in other scenarios explainability is valued equally to or more than accuracy.
\end{enumerate}

\subsection{Accuracy $\times$ Causality}

Accuracy is about how much a machine learning model makes correct guesses, Causality is about studying the causal relationships between variables. We can say that the problem of getting good accuracy lies on the first level of causation, as we are concerned about the data, although causality concepts could be used to transfer what was learned with the data of one population to another, for instance. Another possible relation is evaluating how accurate are causal discovery and causal inference algorithms. In general, these two concepts are unrelated.

\subsection{Accuracy $\times$ QIF}

Acuracy can be viewed as a form of utility, which represents how useful a system is, which might be affected by how vulnerable it is. It is trivial to develop a system without any vulnerability, for instance, a system that always outputs $0$ no matter what is the input. But it's not useful, so we can consider that QIF is related to accuracy when we consider the former as the study of information leakage from a system and the latter as a form of measuring utility.

\subsection{Fairness $\times$ Privacy}

There are some impossibility results in the literature that argue why it is impossible to achieve both pricacy and group fairness constraints under non-trivial accuracy\cite{On the Compatibility of Privacy and Fairness}. We also have positive results that show how stasifying privacy constraints can help mitigating group fairness\cite{A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results}\cite{On the impact of multi-dimensional local differential privacy on fairness}\cite{(Local) Differential Privacy has NO Disparate Impact on Fairness}, and we also have similar positive results results for individual fairness notions\cite{Awareness}.

\begin{enumerate}
\item \cite{A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results} aims to provide theoretical results that relate how training a machine learning model on a dataset in which local differential privacy mechanisms were applied can impact the fairness of this model, if the data-gatherer does not try to reverse the applied noise. The results are provided for a simplified machine learning model, from a theoretical persepctive.
\item \cite{On the impact of multi-dimensional local differential privacy on fairness} aims to evaluate experimentally how training a machine learning model on a multi-dimensional data set in which local differential privacy mechanisms were applied can impact the fairness of the model, again if the data-gatherer does not try to reverse the noise.
\item \cite{(Local) Differential Privacy has NO Disparate Impact on Fairness} executes an empirical evaluation of the impact of many Local Differential Privacy mechanisms on fairness when we do not try to reverse the applied noise, including a new privacy budget allocation scheme based on the domain size of sensitive attributes.
\item \cite{Awareness} introduces a notion of individual fairness that is a generalization of differential privacy, and explores the relationship between this notion of fairness, differential privacy and statistical disparity.
\item \cite{On the Compatibility of Privacy and Fairness} present theoretical results that show the impossibility of having a classifier that is not trivially accurate and at the same time satisfies $(\epsilon,0)$-differential privacy and equality of opportunity constraints. The paper also shows a PAC learner for an approximate fairness definition they provide.
\end{enumerate}

\subsection{Fairness $\times$ Explainability}

Fairness evaluation is a goal usually mentioned when the utility of explainability studies is discussed in the literature\cite{"Just" accuracy? Procedural fairness demands explainability in AI-based medical resource allocations}. There are also some negative results in this respect, that indicate that in some situations, it is possible to provide convincingly fair explanations for decisions that were actually made in an unfair way\cite{How to explain and justify almost any decision: Potential pitfalls for accountability in ai decision-making}.

\begin{enumerate}
\item \cite{"Just" accuracy? Procedural fairness demands explainability in AI-based medical resource allocations} discusses how not providing explanations for decision can threaten accountability, bias avoidance and transparency when evaluating and mitigating unfairness in ML-based decisions.
\item \cite{How to explain and justify almost any decision: Potential pitfalls for accountability in ai decision-making} shows empirical results of training a baseline, a fair, an unfair and a random model on the COMPAS\cite{Compass} dataset and then finding possible explanations for the results provided by the model. The paper explores a space of explanations that are simple, verifiable and relevant according to definitions they provide for these terms, and show that it's possible to provide valid justifications for the decisions for the majority of the models. They also explore the situation in which explanations are provided for many data points and then analysed to verify for consistency, sufficiency and uniqueness (which they define formally in the paper), and achieve similar results: it can be possible to provide justifications for almost any decision that seem valid, even if the justification does not reflect how the model really decides.
\end{enumerate}

\subsection{Fairness $\times$ Causality}

{\color{red} Citar paper Karima sobre noções causais de fairness. Citar tmb THE IMPOSSIBILITY THEOREM OF MACHINE FAIRNESS A CAUSAL PERSPECTIVE, esse segundo tem um problema de considerar equalized odds ao contrário.}

\subsection{Fairness $\times$ QIF}

{\color{red} Citar trabalho do Bruno sobre QIF ao contrário como noção de fairness talvez}

\subsection{Privacy $\times$ Explainability}

{\color{red} No idea!}

\subsection{Privacy $\times$ Causality}

{\color{red} No idea! Maybe be private to causal discovery? Maybe be private but allow causal discovery? I think that there is a paper by Sylvia about this...}

\subsection{Privacy $\times$ QIF}

{\color{red} QIF was desined to work with privacy, quantifying how much the sensitive information is leaking is in a way quantifying privacy.}

\subsection{Explainability $\times$ Causality}

{\color{red} Causality is inheretely easier to explain.}

\subsection{Explainability $\times$ QIF}

{\color{red} No idea! Maybe a way to see which variables leak more information?}

\subsection{Causality $\times$ QIF}

{\color{red} I don't think that the paper I saw was published, but maybe we can mention that.}

