\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[table]{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{steinmetz}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{hyperref}
%\usepackage[hidelinks]{hyperref}
\usepackage{caption}
\usepackage{subcaption}

\setlength{\arrayrulewidth}{0.1mm}
%\setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{2.5}
\newcolumntype{s}{>{\columncolor[HTML]{AAACED}} p{3cm}}
\newcolumntype{r}{p{3mm}}
\newcolumntype{a}{p{10mm}}
%\arrayrulecolor[HTML]{DB5800}
\newcommand{\qm}[1]{``#1"}

\newtheorem{definition}{Definition}


\onecolumn

\title{Relations between Causality, Fairness, Privacy, Accuracy, Information Flow and Explainability in Machine Learning\large\\ Type: Scientific\\Advisor: Mário Sérgio Alvim}
\author{Artur Gaspar da Silva}
\date{31/07/2024}


\begin{document}


\begin{titlepage}
    \begin{center}

        \Huge
        \textbf{Universidade Federal de Minas Gerais}

        \vspace{0.5cm}
        \LARGE
            Department of Computer Science

        \vspace{0.5cm}
        \large
           Undergraduate Thesis, Part I 

        \vspace{0.7cm}

        \includegraphics[width=0.4\textwidth]{logoUFMG.jpg}



        \vspace{0.5cm}

        \Huge
            Relations between Causality, Fairness, Privacy, Accuracy, Information Flow and Explainability in Machine Learning
            \\\large Type: Scientific

        \vspace{0.5cm}
        \begin{abstract}
            Recent years have witnessed an enormous advance in the area of Machine Learning, reflected by the popularity of Artificial Inteligence systems. For most of the history of machine learning research, the main goal was the development of machine learning algorithms that led to more accurate models, but it is now very clear that there are many other important areas to develop. We want models to be fair to unprivileged groups in society, to not reveal private information used in the model training, to provide comprehensible explanations to humans in order to help identifying causal relationships, among many relevant goals other than simply improving model accuracy. This work reviews the literature for the identified relationships among these concepts in Machine Learning.
        \end{abstract}

        \vspace{0.5cm}


     \end{center}

\raggedright

\Large
    Supervisor:
    \vspace{0.2cm}\\
\Large
    Prof. Mário Sérgio Alvim
    \vspace{0.125cm}\\

\raggedleft

\Large
    Thesis written by:
    \vspace{0.125cm}\\
\Large
    Artur Gaspar da Silva



\begin{center}
    \vspace{1cm}
        Academic Semester 2024/01
\end{center}

\end{titlepage}
%\maketitle

{\color{red} Fazer revisão geral gramatical de tudo a partir do background teórico pra conceitos individuais.}
{\color{red} Ajustar o tamanho das imagens}


\newpage
\tableofcontents
\newpage
\twocolumn
\section{Introduction}
%Capítulo introdutório: caracterização do problema, motivação, objetivos, breve descrição da estrutura do trabalho.

Recent research\cite{Sok}\cite{Reductions}\cite{Rachel}\cite{Awareness} indicates numerous tensions and synergies between many concepts that surround the Machine Learning literature, including Fairness, Privacy, Accuracy and Explainability. For instance, there is an inherent tradeoff between Fairness and Accuracy such that, depending on the data distribution, it might be impossible to develop a model that achieves acceptable values for both fairness and accuracy if we consider some reasonable fairness metrics\cite{Carlos}. Also, there has been some work on introducing Causality concepts into the discussion, for instance, to develop better fairness metrics\cite{CausalFair}. It has also been suggested to use naturally interpretable models (as explanations for more complex models) for auditing systems and checking if they are fair, although this might lead to problems\cite{ExplainAll}. This area of research is especially relevant nowadays, given the importance that Machine Learning and Artificial Intelligence systems have: we now have computational systems that are part of processes of making decisions with big impacts on people's lives, for instance, recidivism prediction\cite{Compass}, loan approvals\cite{Loans}, hiring decisions\cite{Jobs}, and others.

In this first part of the Undergraduate Thesis (POC I), we provide a concise review of the literature on the topics presented. The main goal is to provide a solid basis for future work on these topics and identify the known connections among them. Section \ref{sec:method} discusses how and when this work was developed; Section \ref{sec:expResults} discusses the expected results for POC I and POC II; Section \ref{sec:theoRef1} discusses the already developed theoretical work on these areas; Section \ref{sec:theoRef2} discusses connections between them found in the literature; Section \ref{sec:concsFuture} provides conclusions and possible future lines of work. 

\section{Methodology}\label{sec:method}

The methodology applied to this project consists, in general, of reading as many papers on the subject as possible in order to gather what has been produced recently. Also, for developing the necessary theoretical background, the classical book Causality\cite{Causality}, by Judea Pearl, was of utmost importance.

The first two weeks (03/17/2024-03/31/2024) consisted of contacting the advisor, preparing the themes and writing the intial proposal; the next two months (04/01/2024-06/27/2024) consisted of reading the Causality book by Judea Pearl, reading papers on the relevant topics, and preparing the partial pitch; the last month (06/28/2024-07/31/2024) was dedicated to writing the final report and preparing the final pitch, as well as reading more papers on the relevant topics.

\section{Expected Results}\label{sec:expResults}

For the first part of the project (POC I), the expected result is an extensive review of the literature on Causality, Fairness, Privacy, Accuracy and Interpretability in Machine Learning, and the relationships between these concepts. For the second part (POC II), the expected results are the reproduction and verification of the viability of applying the theoretical framework of Quantitative Information Flow to these concepts, with the possibility of developing new theoretical results. This document shows the results of the first part, POC I.

\section{Theoretical Background for Individual Concepts}\label{sec:theoRef1}
%Capítulo referencial: identificação de trabalhos correlatos, referencial teórico.

First, we provide a general background on some introductory concepts in the areas of Machine Learning, Causality, Fairness, Privacy, Explainability in Machine Learning, and also Quantitative Information Flow (QIF).

\input{intros}

\section{Contributions: A Review of the Relations Between the Concepts}\label{sec:theoRef2}

In this section we mention comparisons found in the literature between these areas of research, and discuss some ideas for unexplored relations in the literature.

\input{pairs}

\section{Conclusions and future work}\label{sec:concsFuture}
%Capítulo de fechamento: conclusões e relação de trabalhos futuros.

We can summarize our findings in the following way:

\begin{enumerate}
    \item \textbf{Accuracy $\times$ Fairness:} There is an inherent trade-off between accuracy and some notions of fairness such as Equal Opportunity Difference and Statistical Parity.
    \item \textbf{Accuracy $\times$ Privacy:} Privacy by addition of noise usually reduces accuracy. Privacy by homomorphic encryption does \emph{not} affect accuracy at the cost of greater computational complexity.
    \item \textbf{Accuracy $\times$ Explainability:} In many situations, more interpretability implies less accuracy (as more compex models are harder to explain/interpret).
    \item \textbf{Accuracy $\times$ Causality:} Causality might help in the transference of machine learning model results between distinct populations, and the development with causal basis can reduce how sensible it is to small changes in the data used to train the model.
    \item \textbf{Accuracy $\times$ QIF:} accuracy can be seen as a form of utility, which usually has a trade-off with information leakage, measured by QIF.
    \item \textbf{Fairness $\times$ Privacy:} If the privacy-preserving mechanism is based on noise, then it might help in reducing unfairness according to some measures. Most results use the noisy distribution directly, without first trying to recover the original distribution.
    \item \textbf{Fairness $\times$ Explainability:} One possibility for checking whether a system is fair is to demand explanations for the system's working, but sometimes it is possible to deceive this verification.
    \item \textbf{Fairness $\times$ Causality:} there are many distinct causal fairness notions, each with its own meaning and class of applicable situations.
    \item \textbf{Fairness $\times$ QIF:} Fairness might be measured by \emph{reverse} information flow, how much one can infer from the output by observing the sensitive values of the input instead of how much can be inferred from the input from observing the output.
    \item \textbf{Privacy $\times$ Explainability:} Conceptually, more privacy implies less explaination power as the data points can not be used in explanations. In practice, however, privacy constraints do not seem to have an impact on some of the classical XAI methods.
    \item \textbf{Privacy $\times$ Causality:} Causal interpretations were used to disentangle confusions about Differential Privacy assumptions. We can use causal notions to improve the use of differential privacy budget and some methods make causal discovery/inference harder.
    \item \textbf{Privacy $\times$ QIF:} It is possible to use the Quantitative Information Flow $g$-vulnerability framework to model private information leakage, and there are theoretical results that show relations with differential privacy, with equality for max-vulnerability.
    \item \textbf{Explainability $\times$ Causality:} In many situations, causal explanations the final goal and although classical explainability methods are based only on the data, it might be possible to use them, as basis for further empirical explanation.
    \item \textbf{Explainability $\times$ QIF:} The relationship between these two is largely unexplored. Maybe it's possible to provide explanations based on how much information flows from each feature to the output?
    \item \textbf{Causality $\times$ QIF:} The releationship between these two is largely unexplored too.
\end{enumerate}

There are some possible avenues for future works:

\begin{enumerate}
    \item Using Quantitative Information Flow notions to develop explainability methods.
    \item Using Quantitative Information Flow notions to quantify the flow of information for all types of machine learning attacks.
    \item Explore what happens with the Privacy $\times$ Fairness and Privacy $\times$ Accuracy trade-offs under noisy distributions if we first try to recover the original distribution.
    \item Exploring the relationship between Quantitative Information and causality, maybe by measuring the information flow between variables to see which variable should be observed to increase the expected information gain on other variables, or other relations.
    \item Discover which variables to add noise in order to hinder individual sensitive attribute inference under the least possible privacy budget expense.
    \item Using Quantitative Information Flow to develop privacy results similar to the ones obtained with Differential Privacy, maybe by developing new and useful gain functions.
    \item Combining existing results with modern Learning Theory, especially by using theoretical results related to Deep Learning.
\end{enumerate}

\bibliographystyle{splncs04}
\bibliography{poc1}

% As referências bibliográficas devem ser compostas considerando uma combinação de fontes, entre livros, artigos em periódicos, artigos em anais e fontes eletrônicas, de acordo com as características e a área temática do trabalho. Deve-se procurar ter uma boa diversidade de fontes, que sejam tão atuais quanto possível. Fontes eletrônicas devem ser usadas com critério, já que algumas delas não têm mérito ou qualidade acadêmica. A Internet é uma boa fonte de artigos em meio digital, tanto através de fontes tradicionais como a ACM e a IEEE (acessíveis através do Portal de Periódicos da CAPES), quanto de mecanismos especializados de busca, como o Google Scholar.

\end{document}
