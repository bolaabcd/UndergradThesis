\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[table]{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{steinmetz}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

\setlength{\arrayrulewidth}{0.1mm}
%\setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{2.5}
\newcolumntype{s}{>{\columncolor[HTML]{AAACED}} p{3cm}}
\newcolumntype{r}{p{3mm}}
\newcolumntype{a}{p{10mm}}
%\arrayrulecolor[HTML]{DB5800}
\newcommand{\qm}[1]{``#1"}

\newtheorem{definition}{Definition}


\onecolumn

\title{Relations between Causality, Fairness, Privacy, Accuracy, Information Flow and Explainability in Machine Learning\large\\ Type: Scientific\\Advisor: Mário Sérgio Alvim}
\author{Artur Gaspar da Silva}
\date{05/04/2024}


\begin{document}
\maketitle

{\color{orange} Table of contents depois ou antes do abstract?}

{\color{orange} Precisa ter tabela de imagens, tabelas e etc.?}

{\color{orange} OK citar coisas do Arxiv?}

{\color{orange} Repetição de artigos que cobrem mais de duas áreas? Talvez só mencionar que foi usado na subseção anterior?}

{\color{orange} Coloca \qm{Chapter x of \textbackslash cite\{...\}} ou \qm{\textbackslash cite[Chapter~x]\{...\}}?}

{\color{orange} O modelo padrão é coluna dupla, mas aparentemente várias pessoas fizeram sem ser coluna dupla sem problemas, e as imagens ficam melhores em coluna simples. Compensa trocar pra coluna dupla???}

\begin{abstract}
    Do que podemos falar aqui?
\end{abstract}

\tableofcontents

\section{Introduction}
%Capítulo introdutório: caracterização do problema, motivação, objetivos, breve descrição da estrutura do trabalho.

Recent research\cite{Sok}\cite{Reductions}\cite{Rachel}\cite{Awareness} indicates numerous tensions and synergies between many concepts that surround the Machine Learning literature, including Fairness, Privacy, Accuracy and Explainability. For instance, there is an inherent tradeoff between Fairness and Accuracy such that, depending on the data distribution, it might be impossible to develop a model that achieves acceptable values for both fairness and accuracy if we consider some reasonable fairness metrics\cite{Carlos}. Also, there has been some work on introducing Causality concepts into the discussion, for instance, to develop better fairness metrics\cite{CausalFair}. It has also been suggested to use naturally interpretable models (as explanations for more complex models) for auditing systems and checking if they are fair, although this might lead to problems\cite{ExplainAll}. This area of research is especially relevant nowadays, given the importance that Machine Learning and Artificial Intelligence systems have: we now have computational systems that are part of processes of making decisions with big impacts on people's lives, for instance, recidivism prediction\cite{Compass}, loan approvals\cite{Loans}, hiring decisions\cite{Jobs}, and others.

In this first part of the Undergraduate Thesis, we provide a concise review of the literature on the topics presented. The main goal is to provide a solid basis for future work on these topics and identify the known connections among them. Section \ref{sec:theoRef1} discusses the already developed theoretical work on these areas; Section \ref{sec:theoRef2} discusses connections between them found in the literature; Section \ref{sec:concsFuture} provides conclusions and possible future lines of work. 

\section{Methodology}

The methodology applied to this project consists, in general, of reading as many papers on the subject as possible in order to gather what has been produced recently. Also, for developing the necessary theoretical background, the classical book Causality\cite{Causality}, by Judea Pearl, was of utmost importance.

\section{Expected Results}

For the first part of the project (POC I), the expected result is an extensive review of the literature on Causality, Fairness, Privacy, Accuracy and Interpretability in Machine Learning, and the relationships between these concepts. For the second part (POC II), the expected results are the reproduction and verification of the viability of applying the theoretical framework of Quantitative Information Flow to these concepts, with the possibility of developing new theoretical results. This document shows the results of the first part, POC I.

\section{Theoretical Background for Individual Concepts}\label{sec:theoRef1}
%Capítulo referencial: identificação de trabalhos correlatos, referencial teórico.

First, we provide a general background on some introductory concepts in the areas of Machine Learning, Causality, Fairness, Privacy, Explainability in Machine Learning, and also Quantitative Information Flow (QIF).

\input{intros}

\section{Contributions: A Review of the Relations Between the Concepts}\label{sec:theoRef2}

In this section we mention comparisons found in the literature between these areas of research, and comment some ideas for unexplored relations in the literature.

\input{pairs}

\section{Conclusions and future work}\label{sec:concsFuture}
%Capítulo de fechamento: conclusões e relação de trabalhos futuros.

We can summarize our findings in the following way:

\begin{enumerate}
    \item \textbf{Accuracy $\times$ Fairness:} There is an inherent trade-off between accuracy and some notions of fairness such as Equal Opportunity Difference and Statistical Parity.
    \item \textbf{Accuracy $\times$ Privacy:} Privacy by addition of noise usually reduces accuracy. Privacy by homomorphic encryption does \emph{not} affect accuracy at the cost of greater computational complexity.
    \item \textbf{Accuracy $\times$ Explainability:} In many situations, more interpretability implies less accuracy (as more compex models are harder to explain/interpret).
    \item \textbf{Accuracy $\times$ Causality:} Causality might help in the transference of machine learning model results between distinct populations, and the development with causal basis can reduce how sensible it is to small changes in the data used to train the model.
    \item \textbf{Accuracy $\times$ QIF:} accuracy can be seem as a form of utility, which usually has a trade-off with information leakage, measured by QIF.
    \item \textbf{Fairness $\times$ Privacy:} If the privacy-preserving mechanism is based on noise, then it might help in reducing unfairness according to some measures. Most results use the noisy distribution directly, without first trying to recover the original distribution.
    \item \textbf{Fairness $\times$ Explainability:} One possibility for checking wheter a system is fair is to demand explanations for the system's working, but sometimes it is possible to deceive this verification.
    \item \textbf{Fairness $\times$ Causality:} there are are many distinct causal fairness notions, each with their own meaning and class of applicable situations.
    \item \textbf{Fairness $\times$ QIF:} Fairness might be measured by \emph{reverse} information flow, how much one can infer from the output by observing the sensitive values of the input instead of how much can be inferred from the input from observing the output.
    \item \textbf{Privacy $\times$ Explainability:} Conceptually, more privacy implies less explaination power as the data points can not be used in explanations. In practice, however, privacy constraints do not seem to have an impact on some of the classical XAI methods.
    \item \textbf{Privacy $\times$ Causality:} Causal interpretations were used to disentangle confusions about Differential Privacy assumptions, we can use causal notions to improve the use of differential privacy budget and some methods make causal discovery/inference harder.
    \item \textbf{Privacy $\times$ QIF:} It is possible to use the Quantitative Information Flow $g$-vulnerability framework to model private information leakage, and there are theoretical results that show relations with differential privacy, with equality for max-vulnerability.
    \item \textbf{Explainability $\times$ Causality:} In many situations, causal explanations the final goal and although classical explainability methods are based only on the data, it might be possible to use them as basis for further empirical explanation.
    \item \textbf{Explainability $\times$ QIF:} The relationship between these two is largely unexplored. Maybe it's possible to provide explanations based on how much information flows from each feature to the output?
    \item \textbf{Causality $\times$ QIF:} The releationship between these two ins largely unexplored too. There is some preliminary unpublished work relating QIF and causal discovery, though. {\color{orange} Faz sentido falar isso?}
\end{enumerate}

There are some possibles avenues for future works:

\begin{enumerate}
    \item Using Quantitative Information Flow notions to develop explainability methods.
    \item Using Quantitative Information Flow notions to quantify the flow of information for all types of machine learning attacks.
    \item Explore what happens with the Privacy $\times$ Fairness and Privacy $\times$ Accuracy trade-offs under noisy distributions if we first try to recover the original distribution.
    \item Exploring the relationship between Quantitative Information and causality, maybe by measuring the information flow between variables to see which variable should be observed to increase the expected information gain on other variables, or other relations.
    \item Discover to which variables to add noise in order to hinder individual sensitive attribute inference under the least possible privacy budget expense.
    \item Using Quantitative Information Flow to develop privacy results similar to the ones obtained with Differential Privacy, maybe by developing new and useful gain functions.
    \item Combining existing results to modern Learning Theory, especially by using theoretical results related to Deep Learning.
\end{enumerate}

\bibliographystyle{splncs04}
\bibliography{poc1}

% As referências bibliográficas devem ser compostas considerando uma combinação de fontes, entre livros, artigos em periódicos, artigos em anais e fontes eletrônicas, de acordo com as características e a área temática do trabalho. Deve-se procurar ter uma boa diversidade de fontes, que sejam tão atuais quanto possível. Fontes eletrônicas devem ser usadas com critério, já que algumas delas não têm mérito ou qualidade acadêmica. A Internet é uma boa fonte de artigos em meio digital, tanto através de fontes tradicionais como a ACM e a IEEE (acessíveis através do Portal de Periódicos da CAPES), quanto de mecanismos especializados de busca, como o Google Scholar.

\end{document}
