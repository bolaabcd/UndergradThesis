\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{steinmetz}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{hyperref}

\usepackage[table]{xcolor}
\setlength{\arrayrulewidth}{0.1mm}
%\setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{2.5}
\newcolumntype{s}{>{\columncolor[HTML]{AAACED}} p{3cm}}
\newcolumntype{r}{p{3mm}}
\newcolumntype{a}{p{10mm}}
%\arrayrulecolor[HTML]{DB5800}
\newcommand{\qm}[1]{``#1"}


\renewcommand\refname{Bibliography}


\title{\Huge Research Project - PhD Graduate program - PPGCC - DCC - UFMG}
\date{}

\begin{document}
\maketitle
\vspace{-8em}

\section{Introduction}

Recent resarch\cite{Sok}\cite{Reductions}\cite{Rachel}\cite{Awareness} indicates numerous tensions and synergies between many concepts that surround the Machine Learning literature, including Fairness, Privacy, Accuracy and Interpretability. For instance, there is an inherent tradeoff between Fairness and Accuracy such that, depending on the data distribution, it might be impossible to develop a model that achieves acceptable values for both fairness and accuracy, if we consider some reasonable fairness metrics\cite{Carlos}. Also, there has been some work on introducing Causality concepts into the discussion, for instance, to develop better fairness metrics\cite{CausalFair}. It has also been suggested to use interpretable models (as explanations to more complex models) for auditing systems and checking if they are fair, although this might lead to difficulties in differentiating honest from dishonest explanations\cite{ExplainAll}. This area of research is especially relevant nowadays, given the importance that Machine Learning and Artificial Intelligence systems have: we now have computational systems that are part of processes of making decisions with big impacts on people's lives, for instance, recidivism prediction\cite{Compass}, loan approvals\cite{Loans}, hiring decisions\cite{Jobs}, and others.

Our goal is to identify the theoretical relations between fairness, privacy and Quantitative Information Flow (QIF), which is a theoretical framework derived from Information Theory\cite{smith2009foundations} that aims to quantify the flow of information in a way that encompasses more scenarios than classical Information Theory. More specifically, we aim to model Differential Privacy and Local Differential Privacy with the QIF framework, and prove new theoretical results that establish this relation. Afterwards, we aim to explore the relationships between privacy and fairness metrics, with focus on Equal Opportunity Difference, Statistical Disparity and Conditional Statistical Disparity. Finally, causalily concepts (based on Judea Pearl's work\cite{Causality}) will be explored to provide better mathematical understanding of fairness and privacy in machine learning systems.

\section{Theoretical Reference}

Causality refers to the study of causal relationships between variables, and how to model and infer causal relationships from the combination of domain knowledge and data\cite{Causality}. This area of research has matured a lot in the last $50$ years, with many different approaches still being developed. Fairness in Machine Learning is concerned with measuring how unfair the results provided by Machine Learning models are to certain groups or individuals\cite{FairMeasures}, and improving how fair the models are\cite{FairSolve}. There are tensions between different fairness measures\cite{Impossibility}\cite{FairTensions}. Privacy is concerned with quantifying how much sensitive information leaks about individuals and methods to avoid this information leakage. In Machine Learning settings, the data collection might be hard for information that is considered very sensitive (for instance, whether or not a person regularly uses illegal drugs) and approaches such as Differential Privacy\cite{DP} might improve trust in the data collection. Also, the model itself might allow the identification of individuals and sensitive features, which is not desirable\cite{liu2021machine}. Accuracy is a metric of how many mistakes the Machine Learning model makes, and there are trade-offs between Accuracy and the other concepts presented\cite{Sok}\cite{Carlos}\cite{Rachel}. The area of Interpretability focus on developing Machine Learning models that have human-comprehensible decisions (either directly or to explain the decisions of more complex models), which might be useful when developing these models\cite{ExplDev} and also to help experts with domain knowledge decide when to trust the results presented by the models\cite{ExplainExperts}. Quantitative Information Flow is a general theoretical framework for measuring amounts of information, with a focus on privacy applications but, in principle, a broader scope\cite{QIF}.

Previous work extensively explored the relationships between Fairness, Interpretability and Privacy\cite{Sok}. Other works focus on: relationships between Privacy and Fairness\cite{Awareness}, the relationship between Privacy, Fariness and Accuracy\cite{Rachel}, the feasibility regions of Accuracy and Fairness metrics\cite{Carlos}\cite{Reductions}, and Causality-Aware fairness metrics\cite{CausalFair}. There are also explorations of the relations between Quantitative Information Flow and Fairness\cite{Bruno}.

More specifically to the relation between Differential Privacy and Quantitative Inforamtion Flow, there are important results in the literature. There are works discussing the relations between differential privacy and $g$-vulnerability, including bounds on $g$-leakage as a function of the $\epsilon$ parameter of differential privacy, and the fact that there is no bound on differential privacy as a function of the $g$-vulnerability \cite{alvim2015information}. Also, we have recent work \cite{fernandes2022explaining} discussing how the $\epsilon$ parameter of Differential Privacy is related to max-case $g$-vulnerability: $e^\epsilon$ is exactly the multiplicative max-case channel capacity under a fixed prior. This work also discusses many other theoretical results relating $g$-vulnerability notions with differential privacy. Finally, recent contributions show that it is possible to model the $\epsilon$ parameter of local differential privacy with the Quantitative Information Flow framework \cite{fernandes2024explaining}, and thus show the viability of our pursuit of modeling $(\epsilon,\delta)$-LDP and Differential Privacy.

\section{Methodology}

This work aims to explore the intersection of Information Theory (by the lens of Quantitative Information Flow), Privacy, and Fairness in Machine Learning. The primary goal is to investigate how these concepts can be effectively integrated and applied to ensure that machine learning models are transparent, robust, and fair, while maintaining the privacy of sensitive data.

First, we will conduct a comprehensive literature review to identify what is currently known and unkown about the relationship between the concepts of privacy, fairness and information flow in machine learning systems. The review will aim to highlight gaps in existing theoretical frameworks, particularly in terms of guarantees provided by common fairness and privacy metrics and notions.

Next, we will explore novel theoretical relationships among these areas. We are particularly interested in modeling the concept of $(\epsilon,\delta)-$Local Differential Privacy from the perspective of Quantitative Information Flow, as both the Differential Privacy and the Quantitative Information Flow frameworks propose methods of quantifying privacy, and there are already methods of modeling $\epsilon-LDP$ in the literature\cite{fernandes2022explaining}. 

Additionally, we will explore the fundamental theoretical relationships between fairness and privacy, obtaining, for instance, the exact trade-offs between privacy and fairness constraints. The focus will be theoretical: by rigorous mathematical reasoning we aim to prove theorems that establish these relationships. We will also experiment with machine learning models trained on real-world datasets, testing empirically various privacy and fairness trade-offs, and identifying patterns that could lead to more robust theoretical results.

Finally, through the exploration of causality notions\cite{Causality}, we aim to provide new insights into the meaning of different quantitative notions of information flow, privacy, fairness, and of their relations, ultimately contributing to the creation of more transparent, accountable, and ethical AI systems.

\subsection{Tasks}
The project can be organized into the following tasks:

\begin{enumerate}
\item \textbf{Task 1: Formalization of Local Differential Privacy using concepts from the QIF framework.}

The first step is to formalize the Local Differential Privacy scenario and definitions, including $\epsilon$ and $\delta$ parameters, as limitations in information leakage on Information-Theoretical Channels according to specific $g$-vulnerabilities. Previous work in the literature\cite{fernandes2022explaining} already modeled the $\epsilon$ parameter by using alternative axioms in QIF, which points to the possibility of modeling also the $\delta$ parameter.

\item \textbf{Task 2: Obtain theoretical and empirical trade-offs between privacy and fairness using causal notions.}

Recent research in the literature indicates trade-offs between fairness and privacy in machine learning\cite{arcolezi2023local}\cite{makhlouf2024impact}, but most work focus on the tension between classical fairness and privacy metrics, instead of utilising causal fairness\cite{CausalFair} and privacy\cite{tschantz2020sok} notions, which we aim to do. Also, we will search for real-world examples and execute simulations of plausible examples to illustrate this tension and what it means in practice.

\item \textbf{Task 3: Model differentially-private data obfuscation when variables have different sensibility and utility values.}

One approach to improve user privacy in Machine Learning is to obfuscate (add randomized noise) to the data, such as in Local differential Privacy mechanisms. But in many practical scenarios the data points are composed of many variables, and not all are sensitive or useful. So, we aim to verify the feasibility of applying noise in a way that reduces the privacy violation while preserving utility as much as possible. This is one of the main goals of the QIF framework, so the results of Task 1 might be useful at this point, and causal notions verified at Task 2 can be used to model the utility of variables.

\item \textbf{Task 4: Search for practical scenarios where the results obtained can be applied.}

After the exploration of the concepts of the previous tasks, we will search for real-world scenarios in which the results obtained can be applied, and verify if they improve the analysis and results obtained, when compared to current solutions. This includes not only Machine Learning models currently in use in industry, but also general statistical research, which might be used for business or governamental decisions.
\end{enumerate}

\section{Course plan}
In this section we describe the course plan to fulfill the requirements for obtaining the PhD degree, listing the courses to be taken. The publication of at least two papers on the project and participation in conferences are also expected.

\begin{enumerate}
\item Information Theory
\item Project and Analysis of Algorithms
\item Teaching Internship I
\item Teaching Internship II
\item Cyberscurity 
\item Algebraic Combinatorics
\item Deep Learning
\item Statistical Foundations of Data Science
\item Measure Theory
\end{enumerate}

\section{Cronogram}

\begin{enumerate}
\item 2025/1: \begin{enumerate}
    \item Literature review.
    \item Formalization of Local Differential Privacy using concepts from the QIF framework (Task 1).
    \item Coursework: Information Theory, Project and Analysis of Algorithms, Cyberscurity.
    \end{enumerate}
\item 2025/2: \begin{enumerate}
    \item Literature review.
    \item Formalization of Local Differential Privacy using concepts from the QIF framework (Task 1).
    \item Coursework: Deep Learning, Statistical Foundations of Data Science, Teaching Internship I.
    \end{enumerate}
\item 2026/1:  \begin{enumerate}
    \item Literature review.
    \item Finish Task 1. 
    \item Coursework: Algebraic Combinatorics, Measure Theory, Teaching Internship II.
    \end{enumerate}
\item 2026/2:  \begin{enumerate}
    \item Start writing first paper on obtained results.
    \item Obtain theoretical and empirical trade-offs between privacy and fairness using causal notions (Task 2)
    \item Prepare and defend the dissertation proposal.
    \end{enumerate}
\item 2027/1:  \begin{enumerate}
    \item Finish Task 2.
    \item Submit first paper on obtained results.
    \item Model differentially-private data obfuscation when variables have different sensibility and utility values (Task 3).
    \end{enumerate}
\item 2027/2:  \begin{enumerate}
    \item Finish Task 3
    \item Start writing second paper on obtained results.
    \end{enumerate}
\item 2028/1:  \begin{enumerate}
    \item Search for practical scenarios where the results obtained can be applied (Task 4)
    \item Start writing the final dissertation.
    \item Submit the second paper on the obtained results
    \end{enumerate}
\item 2028/2:  \begin{enumerate}
    \item Finish Task 4
    \item Defend the dissertation.
    \end{enumerate}
\end{enumerate}

\bibliographystyle{splncs04}
\bibliography{poc2}

\end{document}
